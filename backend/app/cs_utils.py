import os
import httpx
import pandas as pd
import json
import pickle
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import asyncio
from dotenv import load_dotenv

print("====[CS_UTILS.PY ì½”ë“œê°€ Dockerì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤]====")

CACHE_EXPIRE_HOURS = 24

def get_cache_directory():
    is_docker = os.getenv('DOCKER_ENV') or os.path.exists('/.dockerenv')
    is_render = os.getenv('RENDER') or os.path.exists('/opt/render')
    
    if is_docker or is_render:
        cache_dir = os.getenv('CACHE_DIR', '/data/cache')
        print(f"[DEBUG] Docker/Render í™˜ê²½ ê°ì§€ë¨ - Persistent Disk ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    else:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        cache_dir = os.path.join(project_root, 'cache')
        print(f"[DEBUG] ë¡œì»¬ í™˜ê²½ - ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir, exist_ok=True)
        print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {cache_dir}")
    
    return cache_dir

CACHE_DIR = get_cache_directory()
print(f"[DEBUG] ìµœì¢… ì„¤ì •ëœ ìºì‹œ ë””ë ‰í† ë¦¬: {CACHE_DIR}")
print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼: {os.listdir(CACHE_DIR) if os.path.exists(CACHE_DIR) else 'ë””ë ‰í† ë¦¬ ì—†ìŒ'}")

load_dotenv()

class ChannelTalkAPI:
    def __init__(self):
        self.base_url = "https://api.channel.io"
        self.access_key = os.getenv("CHANNEL_ACCESS_KEY")
        self.access_secret = os.getenv("CHANNEL_ACCESS_SECRET")
        self.headers = {
            "x-access-key": self.access_key,
            "x-access-secret": self.access_secret,
            "Content-Type": "application/json"
        }

    async def get_all_users(self, limit: int = 100) -> List[Dict]:
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        url = f"{self.base_url}/open/v5/users"
        params = {"limit": limit}
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            data = response.json()
            return data.get('users', [])

    async def get_userchats(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_timestamp = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_timestamp = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)
        
        all_userchats = []
        since = None
        page_count = 0
        max_pages = 10
        collected_ids = set()
        last_next = None
        consecutive_same_next = 0
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/user-chats"
                params = {"limit": limit, "state": "closed"}
                if since:
                    params["since"] = since
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(url, headers=self.headers, params=params)
                    response.raise_for_status()
                    data = response.json()
                    user_chats = data.get('userChats', [])
                    next_value = data.get('next', None)
                    
                    print(f"[API] {page_count}ë²ˆì§¸ | since: {since} | userChats: {len(user_chats)} | next: {next_value}")
                    
                    if not user_chats:
                        print("[API] ë” ì´ìƒ userChats ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    filtered_chats = []
                    for chat in user_chats:
                        chat_id = chat.get("id")
                        if chat_id not in collected_ids:
                            first_asked_at = chat.get("firstAskedAt")
                            if first_asked_at and start_timestamp <= first_asked_at <= end_timestamp:
                                filtered_chats.append(chat)
                                collected_ids.add(chat_id)
                    
                    all_userchats.extend(filtered_chats)
                    print(f"[API] ë‚ ì§œ í•„í„°ë§ í›„ ì¶”ê°€ëœ ì±„íŒ…: {len(filtered_chats)}")
                    
                    if not next_value or not str(next_value).strip():
                        print("[API] next ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    if next_value == last_next:
                        consecutive_same_next += 1
                        print(f"[API] ë™ì¼ next ë°˜ë³µ {consecutive_same_next}íšŒ | next: {next_value}")
                        if consecutive_same_next >= 2:
                            print("[API] ë¬´í•œë£¨í”„ ë°©ì§€, ì¢…ë£Œ")
                            break
                    else:
                        consecutive_same_next = 0
                    
                    if user_chats:
                        latest_chat = user_chats[0]
                        latest_timestamp = latest_chat.get("firstAskedAt")
                        if latest_timestamp and latest_timestamp < start_timestamp:
                            print(f"[API] ìµœì‹  ë°ì´í„°({latest_timestamp})ê°€ ìš”ì²­ ê¸°ê°„({start_timestamp})ë³´ë‹¤ ì´ì „, ì¢…ë£Œ")
                            break
                    
                    since = next_value
                    last_next = next_value
                    
        except httpx.HTTPStatusError as e:
            print(f"HTTP Error: {e.response.status_code} - {e.response.text}")
            raise
        except Exception as e:
            print(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
        
        print(f"[API] ì´ ìˆ˜ì§‘ëœ ì±„íŒ… ìˆ˜: {len(all_userchats)} (ê¸°ê°„: {start_date} ~ {end_date})")
        return all_userchats

    async def get_userchat_by_id(self, userchat_id: str) -> Dict:
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        url = f"{self.base_url}/open/v5/user-chats/{userchat_id}"
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, headers=self.headers)
            response.raise_for_status()
            return response.json()

    async def get_messages(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        """íŠ¹ì • ê¸°ê°„ì˜ ë©”ì‹œì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_timestamp = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_timestamp = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)
        
        all_messages = []
        since = None
        page_count = 0
        max_pages = 10
        collected_ids = set()
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/messages"
                params = {"limit": limit}
                if since:
                    params["since"] = since
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(url, headers=self.headers, params=params)
                    response.raise_for_status()
                    data = response.json()
                    messages = data.get('messages', [])
                    next_value = data.get('next', None)
                    
                    print(f"[MESSAGES API] {page_count}ë²ˆì§¸ | since: {since} | messages: {len(messages)} | next: {next_value}")
                    
                    if not messages:
                        print("[MESSAGES API] ë” ì´ìƒ messages ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    filtered_messages = []
                    for message in messages:
                        message_id = message.get("id")
                        if message_id not in collected_ids:
                            created_at = message.get("createdAt")
                            if created_at and start_timestamp <= created_at <= end_timestamp:
                                filtered_messages.append(message)
                                collected_ids.add(message_id)
                    
                    all_messages.extend(filtered_messages)
                    print(f"[MESSAGES API] ë‚ ì§œ í•„í„°ë§ í›„ ì¶”ê°€ëœ ë©”ì‹œì§€: {len(filtered_messages)}")
                    
                    if not next_value or not str(next_value).strip():
                        print("[MESSAGES API] next ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    since = next_value
                    
        except httpx.HTTPStatusError as e:
            print(f"HTTP Error: {e.response.status_code} - {e.response.text}")
            raise
        except Exception as e:
            print(f"MESSAGES API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
        
        print(f"[MESSAGES API] ì´ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ìˆ˜: {len(all_messages)} (ê¸°ê°„: {start_date} ~ {end_date})")
        return all_messages

    def hms_to_seconds(self, time_str: str) -> int:
        """HH:MM:SS í˜•ì‹ì˜ ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        try:
            hours, minutes, seconds = map(int, time_str.split(':'))
            return hours * 3600 + minutes * 60 + seconds
        except:
            return 0

    def extract_level(self, tags: List[str], type_name: str, level: int) -> Optional[str]:
        """íƒœê·¸ì—ì„œ íŠ¹ì • íƒ€ì…ì˜ ë ˆë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not tags:
            print(f"[EXTRACT] íƒœê·¸ ì—†ìŒ: {type_name}")
            return None
        
        print(f"[EXTRACT] íƒœê·¸ ëª©ë¡: {tags}")
        print(f"[EXTRACT] ì°¾ëŠ” íƒ€ì…: {type_name}, ë ˆë²¨: {level}")
        
        for tag in tags:
            if tag.startswith(f"{type_name}/"):
                parts = tag.split("/")
                if len(parts) > level:
                    result = parts[level]
                    print(f"[EXTRACT] ë§¤ì¹­ íƒœê·¸: {tag}, íŒŒíŠ¸: {parts}, ì¶”ì¶œ ê²°ê³¼: {result}")
                    return result
                else:
                    print(f"[EXTRACT] ë ˆë²¨ ë¶€ì¡±: {len(parts)} <= {level}")
        
        print(f"[EXTRACT] ë§¤ì¹­ íƒœê·¸ ì—†ìŒ: {type_name}")
        return None

    async def process_userchat_data(self, data: List[Dict]) -> pd.DataFrame:
        keep_keys = [
            "userId", "mediumType", "workflowId", "tags", "chats", "createdAt", 
            "firstAskedAt", "operationWaitingTime", "operationAvgReplyTime", 
            "operationTotalReplyTime", "operationResolutionTime"
        ]

        def convert_time(key, ms):
            if ms is None:
                return None
            try:
                if key == "firstAskedAt":
                    # ğŸ’¡ í•­ìƒ pandas.Timestampë¡œ ë³€í™˜!
                    if isinstance(ms, str):
                        return pd.to_datetime(ms, errors='coerce')
                    elif isinstance(ms, (int, float)):
                        return pd.to_datetime(ms, unit='ms')
                    elif isinstance(ms, (pd.Timestamp, datetime)):
                        return ms
                    else:
                        return pd.NaT
                else:
                    from datetime import timedelta
                    td = timedelta(milliseconds=ms)
                    total_seconds = int(td.total_seconds())
                    hours = total_seconds // 3600
                    minutes = (total_seconds % 3600) // 60
                    seconds = total_seconds % 60
                    return f"{hours:02}:{minutes:02}:{seconds:02}"
            except Exception as e:
                print(f"[convert_time] ì˜¤ë¥˜ key={key} value={ms}: {e}")
                return None

        processed_data = []
        
        for idx, item in enumerate(data):
            # firstAskedAt ì¡´ì¬ ì—¬ë¶€ ì²´í¬
            first_asked_at = item.get("firstAskedAt")
            if first_asked_at is None:
                print(f"[PROCESS] ì•„ì´í…œ #{idx} - firstAskedAt ì—†ìŒ, ìŠ¤í‚µ")
                continue
            
            print(f"[PROCESS] ì›ë³¸ ì•„ì´í…œ #{idx} í‚¤ë“¤: {list(item.keys())}")
            tags = item.get('tags', [])
            print(f"[PROCESS] ì•„ì´í…œ #{idx} tags: {tags}")

            new_obj = {}
            for key in keep_keys:
                value = item.get(key)
                if key == "workflowId":
                    # source.workflow.idì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì˜¬ë°”ë¥¸ ê²½ë¡œ)
                    value = item.get("source", {}).get("workflow", {}).get("id")
                elif key in ["firstAskedAt", "operationWaitingTime", "operationAvgReplyTime", "operationTotalReplyTime", "operationResolutionTime"]:
                    value = convert_time(key, value)
                new_obj[key] = value

            ê³ ê°ìœ í˜•_1ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 1)
            ê³ ê°ìœ í˜•_2ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 2)
            ë¬¸ì˜ìœ í˜•_1ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 1)
            ë¬¸ì˜ìœ í˜•_2ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 2)
            ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 1)
            ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 2)

            ê³ ê°ìœ í˜• = f"{ê³ ê°ìœ í˜•_1ì°¨}/{ê³ ê°ìœ í˜•_2ì°¨}" if ê³ ê°ìœ í˜•_1ì°¨ and ê³ ê°ìœ í˜•_2ì°¨ else ê³ ê°ìœ í˜•_1ì°¨
            ë¬¸ì˜ìœ í˜• = f"{ë¬¸ì˜ìœ í˜•_1ì°¨}/{ë¬¸ì˜ìœ í˜•_2ì°¨}" if ë¬¸ì˜ìœ í˜•_1ì°¨ and ë¬¸ì˜ìœ í˜•_2ì°¨ else ë¬¸ì˜ìœ í˜•_1ì°¨
            ì„œë¹„ìŠ¤ìœ í˜• = f"{ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨}/{ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨}" if ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨ and ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ else ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨

            print(f"[PROCESS] ì¶”ì¶œ ê²°ê³¼ - ê³ ê°ìœ í˜•: {ê³ ê°ìœ í˜•}, ë¬¸ì˜ìœ í˜•: {ë¬¸ì˜ìœ í˜•}, ì„œë¹„ìŠ¤ìœ í˜•: {ì„œë¹„ìŠ¤ìœ í˜•}")

            processed_item = {
                **new_obj,
                "ê³ ê°ìœ í˜•": ê³ ê°ìœ í˜•,
                "ë¬¸ì˜ìœ í˜•": ë¬¸ì˜ìœ í˜•,
                "ì„œë¹„ìŠ¤ìœ í˜•": ì„œë¹„ìŠ¤ìœ í˜•,
                "ê³ ê°ìœ í˜•_1ì°¨": ê³ ê°ìœ í˜•_1ì°¨,
                "ë¬¸ì˜ìœ í˜•_1ì°¨": ë¬¸ì˜ìœ í˜•_1ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨,
                "ê³ ê°ìœ í˜•_2ì°¨": ê³ ê°ìœ í˜•_2ì°¨,
                "ë¬¸ì˜ìœ í˜•_2ì°¨": ë¬¸ì˜ìœ í˜•_2ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨,
            }
            processed_data.append(processed_item)

        df = pd.DataFrame(processed_data)

        # firstAskedAt ì»¬ëŸ¼ ì „ì²´ íƒ€ì… ê°•ì œë³€í™˜ (ì§„ì§œ ì•ˆì „í•˜ê²Œ!)
        if "firstAskedAt" in df.columns:
            df["firstAskedAt"] = pd.to_datetime(df["firstAskedAt"], errors="coerce")

        # í•„ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ì—†ìœ¼ë©´ ë¹ˆ ê°’ìœ¼ë¡œ ìƒì„±
        required_columns = [
            "ê³ ê°ìœ í˜•", "ë¬¸ì˜ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•",
            "ê³ ê°ìœ í˜•_1ì°¨", "ë¬¸ì˜ìœ í˜•_1ì°¨", "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨",
            "ê³ ê°ìœ í˜•_2ì°¨", "ë¬¸ì˜ìœ í˜•_2ì°¨", "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨",
            "firstAskedAt", "operationWaitingTime",
            "operationAvgReplyTime", "operationTotalReplyTime",
            "operationResolutionTime"
        ]

        for col in required_columns:
            if col not in df.columns:
                print(f"[PROCESS] í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½, ë¹ˆ ê°’ìœ¼ë¡œ ìƒì„±: {col}")
                df[col] = None
        
        print(f"[PROCESS] ìµœì¢… ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        print(f"[PROCESS] ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(df)} ê±´")

        return df

channel_api = ChannelTalkAPI()

class ServerCache:
    def __init__(self, cache_dir=None):
        if cache_dir is None:
            self.cache_dir = CACHE_DIR
        else:
            self.cache_dir = cache_dir
        print(f"[CACHE] ServerCache ì´ˆê¸°í™” - ìºì‹œ ë””ë ‰í† ë¦¬: {os.path.abspath(self.cache_dir)}")
        self.ensure_cache_dir()
    
    def ensure_cache_dir(self):
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def get_cache_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def get_metadata_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}_metadata.json")
    
    def save_data(self, cache_key: str, data: pd.DataFrame, metadata: Dict):
        print(f"====[save_data ì§„ì…]==== key={cache_key}, ë°ì´í„°ê¸¸ì´={len(data)}")
        print(f"cache_key: {cache_key}")
        print(f"data type: {type(data)}, len: {len(data)}")
        print(f"data columns: {getattr(data, 'columns', 'N/A')}")
        print(f"data.empty: {getattr(data, 'empty', 'N/A')}")
        if hasattr(data, 'head'):
            print(f"data.head():\n{data.head()}")
        
        try:
            self.ensure_cache_dir()
            metadata.update({
                "saved_at": datetime.now().isoformat(),
                "data_count": len(data),
                "cache_version": "1.0"
            })
            
            # ğŸ’¡ firstAskedAt ì»¬ëŸ¼ ì „ì²´ íƒ€ì… ê°•ì œë³€í™˜
            print(f"[DEBUG] 'firstAskedAt' in data.columns: {'firstAskedAt' in data.columns}")
            print(f"[DEBUG] not data.empty: {not data.empty}")
            if "firstAskedAt" in data.columns and not data.empty:
                print(f"[DEBUG] ì›ë³¸ firstAskedAt dtype: {data['firstAskedAt'].dtype}")
                print(f"[DEBUG] ì›ë³¸ firstAskedAt ìƒ˜í”Œ: {data['firstAskedAt'].head(10).tolist()}")
                
                data["firstAskedAt"] = pd.to_datetime(data["firstAskedAt"], errors="coerce")
                print(f"[DEBUG] ë³€í™˜ í›„ firstAskedAt dtype: {data['firstAskedAt'].dtype}")
                print(f"[DEBUG] ë³€í™˜ í›„ firstAskedAt ìƒ˜í”Œ: {data['firstAskedAt'].head(10).tolist()}")
                
                valid_times = data["firstAskedAt"].dropna()
                print(f"[DEBUG] dropna í›„ valid_times ê°œìˆ˜: {len(valid_times)}")
                print(f"[DEBUG] valid_times ìƒ˜í”Œ: {valid_times.head(10).tolist()}")

                def safe_to_iso(val):
                    if isinstance(val, (pd.Timestamp, datetime)):
                        if pd.isna(val):
                            return ""
                        return val.isoformat()
                    if isinstance(val, str):
                        try:
                            dt = pd.to_datetime(val, errors="coerce")
                            if pd.isna(dt):
                                return ""
                            return dt.isoformat()
                        except Exception as e:
                            print(f"[CACHE] safe_to_iso: str ë³€í™˜ ì‹¤íŒ¨ {val}: {e}")
                            return ""
                    return ""
                
                if not valid_times.empty:
                    start_val = valid_times.min()
                    end_val = valid_times.max()
                    print(f"[DEBUG] min: {start_val} (type: {type(start_val)})")
                    print(f"[DEBUG] max: {end_val} (type: {type(end_val)})")
                else:
                    start_val = ""
                    end_val = ""
                    print(f"[DEBUG] valid_timesê°€ ë¹„ì–´ìˆìŒ - ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •")
                
                metadata["first_asked_start"] = safe_to_iso(start_val)
                metadata["first_asked_end"] = safe_to_iso(end_val)
            else:
                print("[DEBUG] firstAskedAt ì¡°ê±´ë¬¸ ì§„ì… ì•ˆë¨ - ì»¬ëŸ¼ ì—†ê±°ë‚˜ ë°ì´í„° ë¹„ì–´ìˆìŒ")

            data_path = self.get_cache_path(cache_key)
            data.to_pickle(data_path)
            metadata_path = self.get_metadata_path(cache_key)
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"[CACHE] ë°ì´í„° ì €ì¥ ì™„ë£Œ: {cache_key}")
            return True
        except Exception as e:
            print(f"[CACHE] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"[CACHE] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def load_data(self, cache_key: str):
        """ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        import json
        import pandas as pd
        data_path = self.get_cache_path(cache_key)
        metadata_path = self.get_metadata_path(cache_key)
        if os.path.exists(data_path) and os.path.exists(metadata_path):
            data = pd.read_pickle(data_path)
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            return data, metadata
        else:
            return None, None
    
    def is_cache_still_valid(self, metadata: dict) -> bool:
        """
        ë©”íƒ€ë°ì´í„°(saved_at ê¸°ì¤€)ë¡œ ìºì‹œ ìœ íš¨ì„± íŒë‹¨.
        - CACHE_EXPIRE_HOURS(ê¸°ë³¸ 24ì‹œê°„) ë‚´ ì €ì¥ëœ ê²ƒì´ë©´ True
        """
        if not metadata or "saved_at" not in metadata:
            return False
        try:
            saved_at = pd.to_datetime(metadata["saved_at"])
            now = pd.Timestamp.now(tz=saved_at.tz if hasattr(saved_at, 'tz') else None)
            diff = (now - saved_at)
            hours = diff.total_seconds() / 3600
            CACHE_EXPIRE_HOURS = 24  # 24ì‹œê°„
            return hours < CACHE_EXPIRE_HOURS
        except Exception as e:
            print(f"[CACHE] is_cache_still_valid ì˜¤ë¥˜: {e}")
            return False

# ìºì‹œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def get_cached_data_month(month: str) -> Optional[pd.DataFrame]:
    """ë‹¨ì¼ ì›” ìºì‹œ ë°ì´í„° ë¡œë“œ"""
    cache_key = f"userchats_{month}"
    df, metadata = server_cache.load_data(cache_key)
    return df

async def get_cached_data(start_date: str, end_date: str, force_refresh: bool = False) -> pd.DataFrame:
    """ê¸°ê°„ ë‚´ ëª¨ë“  ì›” ìºì‹œ ë³‘í•© + í•„ìš”ì‹œ ì™¸ë¶€ API fetch"""
    def _get_required_months(start_date, end_date):
        start_month = pd.to_datetime(start_date).to_period('M')
        end_month = pd.to_datetime(end_date).to_period('M')
        months = []
        current = start_month
        while current <= end_month:
            months.append(str(current))
            current += 1
        return months

    months = _get_required_months(start_date, end_date)
    all_data = []

    for month in months:
        if force_refresh:
            # ê°•ì œ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œ: ìºì‹œ ë¬´ì‹œí•˜ê³  í•­ìƒ APIì—ì„œ ê°€ì ¸ì˜¤ê¸°
            print(f"[FETCH] {month} (ê°•ì œ ìƒˆë¡œê³ ì¹¨) â†’ APIì—ì„œ ë¶ˆëŸ¬ì˜´")
            year, m = map(int, month.split('-'))
            month_start = datetime(year, m, 1)
            if m == 12:
                month_end = datetime(year + 1, 1, 1) - timedelta(days=1)
            else:
                month_end = datetime(year, m + 1, 1) - timedelta(days=1)
            start_str = month_start.strftime("%Y-%m-%d")
            end_str = month_end.strftime("%Y-%m-%d")

            try:
                userchats = await channel_api.get_userchats(start_str, end_str)
                if userchats:
                    df = await channel_api.process_userchat_data(userchats)
                    meta = {
                        "month": month,
                        "range": [start_str, end_str],
                        "api_fetch": True,
                        "force_refresh": True,
                    }
                    server_cache.save_data(f"userchats_{month}", df, meta)
                    print(f"[FETCH] {month}: ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì„±ê³µ, {len(df)}ê±´ ì €ì¥ë¨")
                    all_data.append(df)
                else:
                    print(f"[FETCH] {month}: ê°•ì œ ìƒˆë¡œê³ ì¹¨ API ê²°ê³¼ ì—†ìŒ")
            except Exception as e:
                print(f"[FETCH] {month} ê°•ì œ ìƒˆë¡œê³ ì¹¨ API ì‹¤íŒ¨: {e}")
        else:
            # ì¼ë°˜ ëª¨ë“œ: ìºì‹œ ë¨¼ì € í™•ì¸
            df = get_cached_data_month(month)
            if df is not None and not df.empty:
                print(f"[CACHE] {month}: ìºì‹œì—ì„œ ë¡œë“œë¨ ({len(df)}ê±´)")
                all_data.append(df)
            else:
                # ìºì‹œ íŒŒì¼ì´ ì—†ì„ ë•Œë§Œ ì™¸ë¶€ API fetch
                # ê° ì›”ì˜ ì‹œì‘~ë ë‚ ì§œ êµ¬í•˜ê¸°
                year, m = map(int, month.split('-'))
                month_start = datetime(year, m, 1)
                if m == 12:
                    month_end = datetime(year + 1, 1, 1) - timedelta(days=1)
                else:
                    month_end = datetime(year, m + 1, 1) - timedelta(days=1)
                start_str = month_start.strftime("%Y-%m-%d")
                end_str = month_end.strftime("%Y-%m-%d")

                # ğŸ”¥ ì™¸ë¶€ APIì—ì„œ ë°›ì•„ì˜¤ê¸°!
                print(f"[FETCH] {month} (ìºì‹œì—†ìŒ) â†’ APIì—ì„œ ë¶ˆëŸ¬ì˜´")
                try:
                    userchats = await channel_api.get_userchats(start_str, end_str)
                    if userchats:
                        df = await channel_api.process_userchat_data(userchats)
                        meta = {
                            "month": month,
                            "range": [start_str, end_str],
                            "api_fetch": True,
                        }
                        server_cache.save_data(f"userchats_{month}", df, meta)
                        print(f"[FETCH] {month}: API ì„±ê³µ, {len(df)}ê±´ ì €ì¥ë¨")
                        all_data.append(df)
                    else:
                        print(f"[FETCH] {month}: API ê²°ê³¼ ì—†ìŒ")
                except Exception as e:
                    print(f"[FETCH] {month} API ì‹¤íŒ¨: {e}")

    if not all_data:
        print(f"[CACHE] ì „ì²´ ê¸°ê°„ {start_date}~{end_date}: ë°ì´í„° ì—†ìŒ")
        return pd.DataFrame()

    combined = pd.concat(all_data, ignore_index=True)
    combined['firstAskedAt'] = pd.to_datetime(combined['firstAskedAt'], errors='coerce')
    mask = (combined['firstAskedAt'] >= start_date) & (combined['firstAskedAt'] <= end_date)
    result = combined[mask].reset_index(drop=True)
    print(f"[CACHE] ìµœì¢… ë°˜í™˜: {len(result)}ê±´")
    return result

def get_filtered_df(df: pd.DataFrame, ê³ ê°ìœ í˜•: str = "ì „ì²´", ë¬¸ì˜ìœ í˜•: str = "ì „ì²´", 
                   ì„œë¹„ìŠ¤ìœ í˜•: str = "ì „ì²´", ë¬¸ì˜ìœ í˜•_2ì°¨: str = "ì „ì²´", 
                   ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨: str = "ì „ì²´") -> pd.DataFrame:
    """í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜"""
    if df.empty:
        return df
    
    filtered_df = df.copy()
    
    # ê³ ê°ìœ í˜• í•„í„°ë§
    if ê³ ê°ìœ í˜• != "ì „ì²´" and "ê³ ê°ìœ í˜•" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["ê³ ê°ìœ í˜•"] == ê³ ê°ìœ í˜•]
    
    # ë¬¸ì˜ìœ í˜• í•„í„°ë§
    if ë¬¸ì˜ìœ í˜• != "ì „ì²´" and "ë¬¸ì˜ìœ í˜•" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["ë¬¸ì˜ìœ í˜•"] == ë¬¸ì˜ìœ í˜•]
    
    # ì„œë¹„ìŠ¤ìœ í˜• í•„í„°ë§
    if ì„œë¹„ìŠ¤ìœ í˜• != "ì „ì²´" and "ì„œë¹„ìŠ¤ìœ í˜•" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["ì„œë¹„ìŠ¤ìœ í˜•"] == ì„œë¹„ìŠ¤ìœ í˜•]
    
    # ë¬¸ì˜ìœ í˜•_2ì°¨ í•„í„°ë§
    if ë¬¸ì˜ìœ í˜•_2ì°¨ != "ì „ì²´" and "ë¬¸ì˜ìœ í˜•_2ì°¨" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["ë¬¸ì˜ìœ í˜•_2ì°¨"] == ë¬¸ì˜ìœ í˜•_2ì°¨]
    
    # ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ í•„í„°ë§
    if ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ != "ì „ì²´" and "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨"] == ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨]
    
    return filtered_df

server_cache = ServerCache()
