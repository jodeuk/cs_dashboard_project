import os
import httpx
import pandas as pd
import numpy as np
import json
import pickle
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple
import asyncio
from dotenv import load_dotenv

print("====[CS_UTILS.PY ì½”ë“œê°€ Dockerì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤]====")

CACHE_EXPIRE_HOURS = 24

def get_cache_directory():
    is_docker = os.getenv('DOCKER_ENV') or os.path.exists('/.dockerenv')
    is_render = os.getenv('RENDER') or os.path.exists('/opt/render')
    
    if is_docker or is_render:
        cache_dir = os.getenv('CACHE_DIR', '/data/cache')
        print(f"[DEBUG] Docker/Render í™˜ê²½ ê°ì§€ë¨ - Persistent Disk ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    else:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        cache_dir = os.path.join(project_root, 'cache')
        print(f"[DEBUG] ë¡œì»¬ í™˜ê²½ - ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir, exist_ok=True)
        print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {cache_dir}")
    
    return cache_dir

CACHE_DIR = get_cache_directory()
print(f"[DEBUG] ìµœì¢… ì„¤ì •ëœ ìºì‹œ ë””ë ‰í† ë¦¬: {CACHE_DIR}")
print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼: {os.listdir(CACHE_DIR) if os.path.exists(CACHE_DIR) else 'ë””ë ‰í† ë¦¬ ì—†ìŒ'}")

load_dotenv()

class ChannelTalkAPI:
    def __init__(self):
        self.base_url = "https://api.channel.io"
        self.access_key = os.getenv("CHANNEL_ACCESS_KEY")
        self.access_secret = os.getenv("CHANNEL_ACCESS_SECRET")
        self.headers = {
            "x-access-key": self.access_key,
            "x-access-secret": self.access_secret,
            "Content-Type": "application/json"
        }

    async def _ensure_keys(self):
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    async def get_all_users(self, limit: int = 100) -> List[Dict]:
        await self._ensure_keys()
        url = f"{self.base_url}/open/v5/users"
        params = {"limit": limit}
        async with httpx.AsyncClient(timeout=30.0) as client:
            r = await client.get(url, headers=self.headers, params=params)
            r.raise_for_status()
            return r.json().get("users", [])

    async def get_userchats(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        await self._ensure_keys()
        start_ts = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_ts = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)

        all_userchats, since, page_count = [], None, 0
        max_pages = 10
        collected_ids = set()
        last_next = None
        consecutive_same_next = 0
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/user-chats"
                params = {"limit": limit, "state": "closed"}
                if since:
                    params["since"] = since
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    resp = await client.get(url, headers=self.headers, params=params)
                    resp.raise_for_status()
                    data = resp.json()

                user_chats = data.get("userChats", [])
                next_value = data.get("next")
                
                if not user_chats:
                    break
                
                for chat in user_chats:
                    user_id = chat.get("userId")
                    if user_id and user_id not in collected_ids:
                        all_userchats.append(chat)
                        collected_ids.add(user_id)
                
                if not next_value or not str(next_value).strip():
                    break
                
                if next_value == last_next:
                    consecutive_same_next += 1
                    if consecutive_same_next >= 2:
                        break
                else:
                    consecutive_same_next = 0
                
                if user_chats:
                    latest_ts = user_chats[0].get("firstAskedAt")
                    if latest_ts and latest_ts < start_ts:
                        break
                
                since = next_value
                last_next = next_value
                    
        except Exception as e:
            print(f"[get_userchats] ì˜¤ë¥˜: {e}")
            raise
        
        print(f"[API] ì´ ìˆ˜ì§‘ëœ ì±„íŒ… ìˆ˜: {len(all_userchats)} (ê¸°ê°„: {start_date} ~ {end_date})")
        return all_userchats

    async def get_userchat_by_id(self, userchat_id: str) -> Dict:
        await self._ensure_keys()
        url = f"{self.base_url}/open/v5/user-chats/{userchat_id}"
        async with httpx.AsyncClient(timeout=30.0) as client:
            r = await client.get(url, headers=self.headers)
            r.raise_for_status()
            return r.json()

    async def get_messages(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        await self._ensure_keys()
        start_ts = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_ts = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)

        all_messages, since, page_count = [], None, 0
        max_pages = 10
        collected_ids = set()
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/messages"
                params = {"limit": limit}
                if since:
                    params["since"] = since

                async with httpx.AsyncClient(timeout=30.0) as client:
                    r = await client.get(url, headers=self.headers, params=params)
                    r.raise_for_status()
                    data = r.json()
                    messages = data.get("messages", [])
                    next_value = data.get("next")

                    if not messages:
                        break

                    filtered = []
                    for m in messages:
                        mid = m.get("id")
                        if mid not in collected_ids:
                            created = m.get("createdAt")
                            if created and start_ts <= created <= end_ts:
                                filtered.append(m)
                                collected_ids.add(mid)
                    all_messages.extend(filtered)

                    if not next_value or not str(next_value).strip():
                        break
                    since = next_value
        except Exception as e:
            print(f"[get_messages] ì˜¤ë¥˜: {e}")
            raise

        print(f"[MESSAGES API] ì´ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ìˆ˜: {len(all_messages)} (ê¸°ê°„: {start_date} ~ {end_date})")
        return all_messages

    # â–¼ ì‹ ê·œ: íŠ¹ì • userChatì˜ ë©”ì‹œì§€ ì „ë¶€(pagination) ì¡°íšŒ
    async def get_messages_by_chat(self, user_chat_id: str, limit: int = 500, sort_order: str = "desc") -> List[Dict]:
        await self._ensure_keys()
        url = f"{self.base_url}/open/v5/user-chats/{user_chat_id}/messages"
        since = None
        out: List[Dict] = []
        guard = 0
        try:
            while True:
                params = {"limit": limit, "sortOrder": sort_order}
                if since:
                    params["since"] = since
                async with httpx.AsyncClient(timeout=30.0) as client:
                    r = await client.get(url, headers=self.headers, params=params)
                    r.raise_for_status()
                    data = r.json()
                    msgs = data.get("messages", [])
                    next_value = data.get("next")
                    out.extend(msgs)
                    if not next_value:
                        break
                    since = next_value
                    guard += 1
                    if guard > 50:
                        break
        except Exception as e:
            print(f"[get_messages_by_chat] ì˜¤ë¥˜: chatId={user_chat_id}, {e}")
            raise
        return out

    def hms_to_seconds(self, time_str: str) -> int:
        try:
            hours, minutes, seconds = map(int, time_str.split(':'))
            return hours * 3600 + minutes * 60 + seconds
        except:
            return 0

    def extract_level(self, tags: List[str], type_name: str, level: int) -> Optional[str]:
        if not tags:
            return None
        for tag in tags:
            if tag.startswith(f"{type_name}/"):
                parts = tag.split("/")
                if len(parts) > level:
                    # level 1 = parts[1], level 2 = parts[2] (0ë²ˆì§¸ëŠ” íƒ€ì…ëª…)
                    return parts[level]
        return None

    async def process_userchat_data(self, data: List[Dict]) -> pd.DataFrame:
        keep_keys = [
            "userId", "personId", "mediumType", "workflowId", "tags", "chats", "createdAt", 
            "firstAskedAt", "operationWaitingTime", "operationAvgReplyTime", 
            "operationTotalReplyTime", "operationResolutionTime"
        ]

        def convert_time(key, ms):
            if ms is None:
                return None
            try:
                if key == "firstAskedAt":
                    if isinstance(ms, str):
                        return pd.to_datetime(ms, errors='coerce')
                    elif isinstance(ms, (int, float)):
                        return pd.to_datetime(ms, unit='ms')
                    elif isinstance(ms, (pd.Timestamp, datetime)):
                        return ms
                    else:
                        return pd.NaT
                else:
                    td = timedelta(milliseconds=ms)
                    total_seconds = int(td.total_seconds())
                    hours = total_seconds // 3600
                    minutes = (total_seconds % 3600) // 60
                    seconds = total_seconds % 60
                    return f"{hours:02}:{minutes:02}:{seconds:02}"
            except Exception:
                return None

        processed_data = []
        for item in data:
            first_asked_at = item.get("firstAskedAt")
            if first_asked_at is None:
                continue
            
            tags = item.get('tags', [])
            new_obj = {}
            for key in keep_keys:
                value = item.get(key)
                if key == "workflowId":
                    value = item.get("source", {}).get("workflow", {}).get("id")
                elif key in ["firstAskedAt", "operationWaitingTime", "operationAvgReplyTime", "operationTotalReplyTime", "operationResolutionTime"]:
                    value = convert_time(key, value)
                new_obj[key] = value

            # íƒœê·¸ ì²˜ë¦¬ ê³¼ì • ë¡œê¹…
            print(f"[TAGS] ì•„ì´í…œ íƒœê·¸: {tags}")

            ê³ ê°ìœ í˜•_1ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 1)
            ê³ ê°ìœ í˜•_2ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 2)
            ë¬¸ì˜ìœ í˜•_1ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 1)
            ë¬¸ì˜ìœ í˜•_2ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 2)
            ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 1)
            ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 2)

            # 1ì°¨ ë¶„ë¥˜ë§Œ ì‚¬ìš© (2ì°¨ ë¶„ë¥˜ ì œê±°)
            ê³ ê°ìœ í˜• = ê³ ê°ìœ í˜•_1ì°¨
            ë¬¸ì˜ìœ í˜• = ë¬¸ì˜ìœ í˜•_1ì°¨
            ì„œë¹„ìŠ¤ìœ í˜• = ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨
            
            print(f"[EXTRACT] ë¬¸ì˜ìœ í˜•: {ë¬¸ì˜ìœ í˜•} (1ì°¨: {ë¬¸ì˜ìœ í˜•_1ì°¨}, 2ì°¨: {ë¬¸ì˜ìœ í˜•_2ì°¨})")

            processed_item = {
                **new_obj,
                "ê³ ê°ìœ í˜•": ê³ ê°ìœ í˜•,
                "ë¬¸ì˜ìœ í˜•": ë¬¸ì˜ìœ í˜•,
                "ì„œë¹„ìŠ¤ìœ í˜•": ì„œë¹„ìŠ¤ìœ í˜•,
                "ê³ ê°ìœ í˜•_1ì°¨": ê³ ê°ìœ í˜•_1ì°¨,
                "ë¬¸ì˜ìœ í˜•_1ì°¨": ë¬¸ì˜ìœ í˜•_1ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨,
                "ê³ ê°ìœ í˜•_2ì°¨": ê³ ê°ìœ í˜•_2ì°¨,
                "ë¬¸ì˜ìœ í˜•_2ì°¨": ë¬¸ì˜ìœ í˜•_2ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨,
                # userChat id
                "userChatId": item.get("id") or item.get("chatId") or item.get("mainKey", "").replace("userChat-", "")
            }
            processed_data.append(processed_item)

        df = pd.DataFrame(processed_data)
        if "firstAskedAt" in df.columns:
            df["firstAskedAt"] = pd.to_datetime(df["firstAskedAt"], errors="coerce")

        required_columns = [
            "ê³ ê°ìœ í˜•", "ë¬¸ì˜ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•",
            "ê³ ê°ìœ í˜•_1ì°¨", "ë¬¸ì˜ìœ í˜•_1ì°¨", "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨",
            "ê³ ê°ìœ í˜•_2ì°¨", "ë¬¸ì˜ìœ í˜•_2ì°¨", "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨",
            "firstAskedAt", "operationWaitingTime",
            "operationAvgReplyTime", "operationTotalReplyTime",
            "operationResolutionTime", "userChatId", "userId", "personId"
        ]
        for col in required_columns:
            if col not in df.columns:
                df[col] = None

        return df

    # â–¼ ì‹ ê·œ: ë©”ì‹œì§€ ë°°ì—´ì—ì„œ CSAT ì„¤ë¬¸ ì¶”ì¶œ
    def extract_csat_from_messages(self, msgs: List[Dict], allowed_trigger_ids: Optional[set] = None) -> Dict:
        """
        ë°˜í™˜ ì˜ˆì‹œ:
        {
          "A-1": 4, "A-2": 3, "comment_3": "í…ìŠ¤íŠ¸",
          "A-4": 4, "A-5": 4, "comment_6": "í…ìŠ¤íŠ¸",
          "csatSubmittedAt": "2025-08-07T16:47:45+09:00",
          "personId": "person_123"
        }
        """
        result = {}
        latest_submit_ts = None
        person_id = None

        def norm_label(label: str) -> Optional[str]:
            if not isinstance(label, str):
                return None
            s = label.strip()
            # ìˆ«ì. ë¡œ ì‹œì‘í•˜ëŠ” ë ˆì´ë¸” â†’ A-n ë§¤í•‘
            # 1. ìƒë‹´ì›ì˜ ì¹œì ˆë„..., 2. ë¬¸ì œ í•´ê²°..., 4. ê¸°ëŠ¥ ì•ˆì •ì„±..., 5. ë””ìì¸...
            if s.startswith("1."): return "A-1"
            if s.startswith("2."): return "A-2"
            if s.startswith("4."): return "A-4"
            if s.startswith("5."): return "A-5"
            # 3/6 í…ìŠ¤íŠ¸ ì‘ë‹µì€ ì½”ë©˜íŠ¸ í‚¤
            if s.startswith("3."): return "comment_3"
            if s.startswith("6."): return "comment_6"
            # ì—…ë¡œë“œ Excelê³¼ ë§ì¶”ê¸° ìœ„í•œ ë°±ì—… ë¼ë²¨
            if "ì¹œì ˆë„" in s: return "A-1"
            if "ë¬¸ì œ í•´ê²°" in s: return "A-2"
            if "ì•ˆì •ì„±" in s: return "A-4"
            if "ë””ìì¸" in s: return "A-5"
            return None

        for m in msgs:
            # â–¼ ì¶”ê°€: triggerId(= workflow íŠ¸ë¦¬ê±°) ìš°ì„  í•„í„°
            if allowed_trigger_ids:
                log = m.get("log") or {}
                trig_ok = (log.get("triggerType") == "workflow" and str(log.get("triggerId")) in allowed_trigger_ids)

                # ë³´ìˆ˜ì ìœ¼ë¡œ, message.workflow.id ë„ í—ˆìš© (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë§¤ì¹˜ë˜ë©´ í†µê³¼)
                wf = (m.get("workflow") or {}).get("id")
                wf_ok = (wf is not None and str(wf) in allowed_trigger_ids)

                if not (trig_ok or wf_ok):
                    continue

            # personId ì¶”ì¶œ (personType: "user"ì¸ ë©”ì‹œì§€ì—ì„œë§Œ)
            if person_id is None and m.get("personType") == "user":
                person_id = m.get("personId") or m.get("person", {}).get("id")

            form = m.get("form")
            if not form:
                continue
            inputs = form.get("inputs") or []
            submitted_at = form.get("submittedAt")
            if submitted_at:
                latest_submit_ts = max(latest_submit_ts or submitted_at, submitted_at)
            for inp in inputs:
                label = norm_label(inp.get("label"))
                if not label:
                    continue
                val = inp.get("value")
                if label.startswith("A-"):
                    try:
                        result[label] = int(val) if val is not None else None
                    except Exception:
                        result[label] = None
                elif label.startswith("comment_"):
                    result[label] = val if isinstance(val, str) else None

        if latest_submit_ts:
            try:
                # API timestamp(ms) â†’ ISO with KST
                kst = timezone(timedelta(hours=9))
                dt = datetime.fromtimestamp(latest_submit_ts/1000, tz=kst)
                result["csatSubmittedAt"] = dt.isoformat()
            except Exception:
                pass
        
        # personId ì¶”ê°€
        if person_id:
            result["personId"] = person_id
            
        return result

    # â–¼ ì‹ ê·œ: CSAT ë°ì´í„°ê°€ ìˆëŠ” userChatë§Œ í•„í„°ë§
    def filter_csat_userchats(self, userchats: List[Dict]) -> List[Dict]:
        """CSAT ë°ì´í„°ê°€ ìˆëŠ” userChatë§Œ í•„í„°ë§ (workflowId: 768201)"""
        csat_chats = []
        for chat in userchats:
            # workflowId í™•ì¸
            workflow_id = chat.get("source", {}).get("workflow", {}).get("id")
            if workflow_id == "768201":
                csat_chats.append(chat)
        return csat_chats

channel_api = ChannelTalkAPI()

class ServerCache:
    def __init__(self, cache_dir=None):
        self.cache_dir = cache_dir or CACHE_DIR
        print(f"[CACHE] ServerCache ì´ˆê¸°í™” - ìºì‹œ ë””ë ‰í† ë¦¬: {os.path.abspath(self.cache_dir)}")
        self.ensure_cache_dir()
    
    def ensure_cache_dir(self):
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir, exist_ok=True)
    
    def get_cache_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def get_metadata_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}_metadata.json")
    
    def save_data(self, cache_key: str, data: pd.DataFrame, metadata: Dict):
        try:
            self.ensure_cache_dir()
            metadata.update({
                "saved_at": datetime.now().isoformat(),
                "data_count": len(data),
                "cache_version": "1.1"
            })
            if "firstAskedAt" in data.columns and not data.empty:
                data["firstAskedAt"] = pd.to_datetime(data["firstAskedAt"], errors="coerce")
                valid = data["firstAskedAt"].dropna()
                def safe_iso(x):
                    if isinstance(x, (pd.Timestamp, datetime)):
                        return x.isoformat()
                    try:
                        return pd.to_datetime(x, errors="coerce").isoformat()
                    except Exception:
                        return ""
                metadata["first_asked_start"] = safe_iso(valid.min()) if not valid.empty else ""
                metadata["first_asked_end"] = safe_iso(valid.max()) if not valid.empty else ""
            data_path = self.get_cache_path(cache_key)
            data.to_pickle(data_path)
            meta_path = self.get_metadata_path(cache_key)
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"[CACHE] ì €ì¥ ì™„ë£Œ: {cache_key} ({len(data)} rows)")
            return True
        except Exception as e:
            print(f"[CACHE] save_data ì‹¤íŒ¨: {e}")
            return False
    
    def load_data(self, cache_key: str):
        data_path = self.get_cache_path(cache_key)
        meta_path = self.get_metadata_path(cache_key)
        if os.path.exists(data_path) and os.path.exists(meta_path):
            df = pd.read_pickle(data_path)
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
            return df, meta
        return None, None

    def clear_all_cache(self) -> bool:
        try:
            for fn in os.listdir(self.cache_dir):
                if fn.endswith(".pkl") or fn.endswith("_metadata.json"):
                    os.remove(os.path.join(self.cache_dir, fn))
            return True
        except Exception as e:
            print(f"[CACHE] ì „ì²´ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def is_cache_still_valid(self, metadata: dict) -> bool:
        if not metadata or "saved_at" not in metadata:
            return False
        try:
            saved_at = pd.to_datetime(metadata["saved_at"])
            now = pd.Timestamp.now(tz=saved_at.tz if hasattr(saved_at, 'tz') else None)
            hours = (now - saved_at).total_seconds() / 3600
            return hours < CACHE_EXPIRE_HOURS
        except Exception:
            return False

    def filter_data_by_date_range(self, df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            first = pd.to_datetime(df['firstAskedAt'], errors='coerce')
            if getattr(first.dt, "tz", None) is not None:
                first = first.dt.tz_convert('Asia/Seoul').dt.tz_localize(None)

            start_kst = pd.to_datetime(start_date)
            end_kst = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)
            mask = (first >= start_kst) & (first <= end_kst)
            out = df.loc[mask].copy()
            return out
        except Exception as e:
            print(f"[CACHE] ë‚ ì§œ ë²”ìœ„ í•„í„° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
server_cache = ServerCache()

# === ìºì‹œ ë³‘í•© ìœ í‹¸ ===
def get_cached_data_month(month: str) -> Optional[pd.DataFrame]:
    cache_key = f"userchats_{month}"
    df, _ = server_cache.load_data(cache_key)
    return df

async def get_cached_data(start_date: str, end_date: str, refresh_mode: str = "cache") -> pd.DataFrame:
    """
    refresh_mode:
    - "cache": ê¸°ì¡´ ìºì‹œë§Œ ì‚¬ìš© (ê¸°ë³¸ê°’)
    - "update": ê¸°ì¡´ ìºì‹œ ìœ ì§€ + ëˆ„ë½ëœ ê¸°ê°„ë§Œ API í˜¸ì¶œ
    - "refresh": ê¸°ì¡´ ìºì‹œ ì™„ì „ ì‚­ì œ + ì „ì²´ ìƒˆë¡œ ìˆ˜ì§‘
    """
    def _months(s, e):
        sm = pd.to_datetime(s).to_period('M')
        em = pd.to_datetime(e).to_period('M')
        cur = sm
        out = []
        while cur <= em:
            out.append(str(cur))
            cur += 1
        return out

    months = _months(start_date, end_date)
    all_data = []
    
    for month in months:
        if refresh_mode == "refresh":
            # ì „ì²´ ê°±ì‹ : ê¸°ì¡´ ìºì‹œ ë¬´ì‹œí•˜ê³  API í˜¸ì¶œ
            print(f"[REFRESH] {month} â†’ userchats API í˜¸ì¶œ í›„ ì €ì¥")
            year, m = map(int, month.split('-'))
            start = datetime(year, m, 1).strftime("%Y-%m-%d")
            if m == 12:
                end = datetime(year+1, 1, 1) - timedelta(days=1)
            else:
                end = datetime(year, m+1, 1) - timedelta(days=1)
            end = end.strftime("%Y-%m-%d")
            try:
                userchats = await channel_api.get_userchats(start, end)
                if userchats:
                    df = await channel_api.process_userchat_data(userchats)
                    meta = {"month": month, "range": [start, end], "api_fetch": True}
                    server_cache.save_data(f"userchats_{month}", df, meta)
                    all_data.append(df)
            except Exception as e:
                print(f"[REFRESH] {month} ì‹¤íŒ¨: {e}")
        elif refresh_mode == "update":
            # ìµœì‹ í™”: ê¸°ì¡´ ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ API í˜¸ì¶œ
            df = get_cached_data_month(month)
            year, m = map(int, month.split('-'))
            current_year = datetime.now().year
            current_month = datetime.now().month
            
            if df is not None and not df.empty:
                if year == current_year and m == current_month:
                    # í˜„ì¬ ë‹¬(8ì›”): ê¸°ì¡´ ìºì‹œ + ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
                    print(f"[UPDATE] {month} í˜„ì¬ ë‹¬ â†’ ê¸°ì¡´ ìºì‹œ({len(df)} rows) + ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€")
                    start = datetime(year, m, 1).strftime("%Y-%m-%d")
                    if m == 12:
                        end = datetime(year+1, 1, 1) - timedelta(days=1)
                    else:
                        end = datetime(year, m+1, 1) - timedelta(days=1)
                    end = end.strftime("%Y-%m-%d")
                    try:
                        userchats = await channel_api.get_userchats(start, end)
                        if userchats:
                            new_df = await channel_api.process_userchat_data(userchats)
                            # ê¸°ì¡´ ë°ì´í„° + ìƒˆë¡œìš´ ë°ì´í„° í•©ì¹˜ê¸°
                            combined_df = pd.concat([df, new_df], ignore_index=True)
                            # ì¤‘ë³µ ì œê±° (userChatId ê¸°ì¤€ - ê° ë¬¸ì˜ë§ˆë‹¤ ê³ ìœ )
                            combined_df = combined_df.drop_duplicates(subset=['userChatId'], keep='first')
                            # ì—…ë°ì´íŠ¸ëœ ìºì‹œ ì €ì¥
                            meta = {"month": month, "range": [start, end], "api_fetch": True, "updated": True}
                            server_cache.save_data(f"userchats_{month}", combined_df, meta)
                            print(f"[UPDATE] {month} ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(df)} â†’ {len(combined_df)} rows")
                            all_data.append(combined_df)
                        else:
                            print(f"[UPDATE] {month} ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ, ê¸°ì¡´ ìºì‹œ ì‚¬ìš©")
                            all_data.append(df)
                    except Exception as e:
                        print(f"[UPDATE] {month} í˜„ì¬ ë‹¬ ê°±ì‹  ì‹¤íŒ¨: {e}, ê¸°ì¡´ ìºì‹œ ì‚¬ìš©")
                        all_data.append(df)
                else:
                    # ê³¼ê±° ë‹¬: ê¸°ì¡´ ìºì‹œ ì‚¬ìš©
                    print(f"[UPDATE] {month} ê³¼ê±° ë‹¬ â†’ ìºì‹œ ì‚¬ìš© ({len(df)} rows)")
                    all_data.append(df)
            else:
                # ìºì‹œê°€ ì—†ëŠ” ê²½ìš°: API í˜¸ì¶œ
                print(f"[UPDATE] {month} ìºì‹œ ì—†ìŒ â†’ API í˜¸ì¶œ")
                start = datetime(year, m, 1).strftime("%Y-%m-%d")
                if m == 12:
                    end = datetime(year+1, 1, 1) - timedelta(days=1)
                else:
                    end = datetime(year, m+1, 1) - timedelta(days=1)
                end = end.strftime("%Y-%m-%d")
                try:
                    userchats = await channel_api.get_userchats(start, end)
                    if userchats:
                        df = await channel_api.process_userchat_data(userchats)
                        meta = {"month": month, "range": [start, end], "api_fetch": True}
                        server_cache.save_data(f"userchats_{month}", df, meta)
                        all_data.append(df)
                except Exception as e:
                    print(f"[UPDATE] {month} API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        else:  # refresh_mode == "cache"
            # ìºì‹œë§Œ ì‚¬ìš©: API í˜¸ì¶œ ì•ˆ í•¨
            df = get_cached_data_month(month)
            if df is not None and not df.empty:
                print(f"[CACHE] {month} ë¡œë“œ ({len(df)} rows)")
                all_data.append(df)
            else:
                print(f"[CACHE] {month} ì—†ìŒ (API í˜¸ì¶œ ì•ˆ í•¨)")
                
    print(f"[DEBUG] all_data ê°œìˆ˜: {len(all_data)}")
    print(f"[DEBUG] all_data ê° ì›”ë³„ ê°œìˆ˜: {[len(df) if df is not None else 0 for df in all_data]}")
    
    if not all_data:
        print(f"[DEBUG] all_dataê°€ ë¹„ì–´ìˆìŒ - ë¹ˆ DataFrame ë°˜í™˜")
        return pd.DataFrame()
    
    print(f"[DEBUG] pd.concat ì‹œì‘ - ì´ {sum(len(df) if df is not None else 0 for df in all_data)} rows)")
    try:
        combined = pd.concat(all_data, ignore_index=True)
        print(f"[DEBUG] pd.concat ì™„ë£Œ: {len(combined)} rows")
    except Exception as e:
        print(f"[ERROR] pd.concat ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()
    
    print(f"[DEBUG] firstAskedAt ì»¬ëŸ¼ ë³€í™˜ ì‹œì‘")
    try:
        combined['firstAskedAt'] = pd.to_datetime(combined['firstAskedAt'], errors='coerce')
        print(f"[DEBUG] firstAskedAt ë³€í™˜ ì™„ë£Œ: {combined['firstAskedAt'].notna().sum()}ê°œ ìœ íš¨, {combined['firstAskedAt'].isna().sum()}ê°œ NaN")
    except Exception as e:
        print(f"[ERROR] firstAskedAt ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()
    
    print(f"[DEBUG] ì¤‘ë³µ ì œê±° ì‹œì‘")
    try:
        before_dedup = len(combined)
        if 'userChatId' in combined.columns:
            combined = combined.drop_duplicates(subset=['userChatId'], keep='first')
        else:
            combined = combined.drop_duplicates(subset=['userId', 'firstAskedAt', 'createdAt'], keep='first')
        after_dedup = len(combined)
        print(f"[DEBUG] ì¤‘ë³µ ì œê±° ì™„ë£Œ: {before_dedup} â†’ {after_dedup} rows")
    except Exception as e:
        print(f"[ERROR] ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()

    print(f"[DEBUG] ë‚ ì§œ í•„í„°ë§ ì‹œì‘")
    try:
        # 1) firstAskedAt â†’ KST naive ë¡œ ì •ê·œí™”
        fa = pd.to_datetime(combined['firstAskedAt'], errors='coerce')
        # tz-aware(ì˜ˆ: UTC) ì¸ ê²½ìš°ë§Œ Asia/Seoulë¡œ ë³€í™˜ í›„ tz ì œê±°
        if getattr(fa.dt, "tz", None) is not None:
            fa = fa.dt.tz_convert('Asia/Seoul').dt.tz_localize(None)
        combined['firstAskedAt'] = fa

        # 2) ë¹„êµ ë²”ìœ„ (KST naive)
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)

        # 3) ë””ë²„ê·¸: íŠ¹ì • ë‚ ì§œ ì ê²€(ì›í•˜ì‹œë©´ ë‚ ì§œ ë°”ê¿”ì„œ ë³´ì„¸ìš”)
        # íŠ¹ì • ì¼ì ì¹´ìš´íŠ¸(ì˜ˆ: 2025-08-19)
        if start_date == end_date:
            target_date = pd.to_datetime(start_date).date()
            same_day = (combined['firstAskedAt'].dt.date == target_date).sum()
            print(f"[DEBUG] same-day({target_date}) rows(before mask): {same_day}")

        print(f"[DEBUG] ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date} (KST naive) â†’ {start_dt} ~ {end_dt}")
        mask = (combined['firstAskedAt'] >= start_dt) & (combined['firstAskedAt'] <= end_dt)
        print(f"[DEBUG] ë§ˆìŠ¤í¬ ì ìš©: {mask.sum()}ê°œ True, {len(mask) - mask.sum()}ê°œ False")
        
        result = combined[mask].reset_index(drop=True)
        print(f"[FILTER] ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: {start_date} ~ {end_date} (KST) â†’ {start_dt} ~ {end_dt} (KST)")
        print(f"[FILTER] í•„í„°ë§ ì „: {len(combined)} rows, í•„í„°ë§ í›„: {len(result)} rows")
        
    except Exception as e:
        print(f"[ERROR] ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()
    
    # ğŸ”§ refresh_mode="refresh"ì¼ ë•Œë§Œ CSAT ìºì‹œë„ í•¨ê»˜ ê°±ì‹ 
    if refresh_mode == "refresh":
        print(f"[REFRESH] CSAT ìºì‹œë„ í•¨ê»˜ ê°±ì‹  ì‹œì‘...")
        try:
            csat_count = await build_and_cache_csat_rows(start_date, end_date)
            print(f"[REFRESH] CSAT ìºì‹œ ê°±ì‹  ì™„ë£Œ: {csat_count} rows")
        except Exception as e:
            print(f"[REFRESH] CSAT ìºì‹œ ê°±ì‹  ì‹¤íŒ¨: {e}")
    
    return result

# === ì‹ ê·œ: CSAT ìºì‹œ ë¹Œë“œ ===
async def build_and_cache_csat_rows(start_date: str, end_date: str) -> int:
    """
    userchats ìºì‹œ(í˜¹ì€ ë°©ê¸ˆ ìƒˆë¡œê³ ì¹¨ëœ ë°ì´í„°)ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
    ê° userChatì˜ messagesë¥¼ ì¡°íšŒ â†’ CSAT ì„¤ë¬¸ íŒŒì‹± â†’ ì›”ë³„ csat ìºì‹œ ì €ì¥.
    ë°˜í™˜: ì €ì¥ëœ row ì´ ê°œìˆ˜
    """
    # 1) ì‚¬ìš©ì ë¬¸ì˜ ë°ì´í„° í™•ë³´(ìºì‹œ ê¸°ì¤€, API í˜¸ì¶œ ì—†ì´ ìš°ì„  ì‹œë„)
    user_df = await get_cached_data(start_date, end_date, refresh_mode="cache")
    if user_df is None or user_df.empty:
        # ê·¸ë˜ë„ ì—†ë‹¤ë©´, ì´ í•¨ìˆ˜ëŠ” ìƒˆë¡œê³ ì¹¨ ê²½ë¡œì—ì„œë§Œ í˜¸ì¶œí•˜ë„ë¡ ì„¤ê³„í–ˆìœ¼ë‹ˆ
        # ê°•ì œ ìƒˆë¡œê³ ì¹¨(=API í˜¸ì¶œ)ë¡œ í•œë²ˆ ì±„ì›Œì¤€ë‹¤.
        user_df = await get_cached_data(start_date, end_date, refresh_mode="refresh")
        if user_df is None or user_df.empty:
            return 0

    # 2) CSAT ë°ì´í„°ê°€ ìˆëŠ” userChatë§Œ í•„í„°ë§ (triggerId: 768201)
    print(f"[CSAT] ì „ì²´ userChat ìˆ˜: {len(user_df)}")
    
    # userChat ë ˆë²¨ì—ì„œëŠ” workflowIdê°€ ì•„ë‹ˆë¼ triggerIdë¥¼ í™•ì¸í•´ì•¼ í•¨
    # í•˜ì§€ë§Œ userChat ìì²´ì—ëŠ” triggerIdê°€ ì—†ê³ , ë©”ì‹œì§€ ë ˆë²¨ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥
    # ë”°ë¼ì„œ ëª¨ë“  userChatì„ ëŒ€ìƒìœ¼ë¡œ í•˜ê³ , ë©”ì‹œì§€ ë ˆë²¨ì—ì„œ í•„í„°ë§
    csat_df = user_df.copy()
    print(f"[CSAT] ì „ì²´ userChat ëŒ€ìƒ (ë©”ì‹œì§€ ë ˆë²¨ì—ì„œ triggerId í•„í„°ë§)")

    # 3) ìµœê·¼ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì •ë ¬ (íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´)
    csat_df = csat_df.sort_values("firstAskedAt", ascending=False)
    
    # 4) chatId ëª©ë¡ê³¼ userId/firstAskedAt ë§¤í•‘
    need_cols = ["userChatId", "userId", "firstAskedAt"]
    for c in need_cols:
        if c not in csat_df.columns:
            csat_df[c] = None
    sub = csat_df[need_cols].dropna(subset=["userChatId"]).drop_duplicates()

    # 5) ê° chatIdì˜ ë©”ì‹œì§€ ì¡°íšŒ & CSAT íŒŒì‹±
    rows = []
    processed_count = 0
    for _, row in sub.iterrows():
        chat_id = str(row["userChatId"])
        user_id = str(row["userId"]) if pd.notna(row["userId"]) else None
        asked_at = pd.to_datetime(row["firstAskedAt"], errors="coerce")
        if not chat_id or pd.isna(asked_at):
            continue

        try:
            print(f"[CSAT] ì²˜ë¦¬ ì¤‘: {chat_id} ({processed_count + 1}/{len(sub)})")
            msgs = await channel_api.get_messages_by_chat(chat_id, limit=500, sort_order="desc")
            cs = channel_api.extract_csat_from_messages(msgs, allowed_trigger_ids={'768201'})
            if not cs:
                print(f"[CSAT] {chat_id}: CSAT ë°ì´í„° ì—†ìŒ")
                continue
            print(f"[CSAT] {chat_id}: CSAT ë°ì´í„° ë°œê²¬ - {list(cs.keys())}")
            
            rows.append({
                "firstAskedAt": asked_at,
                "userId": user_id,
                "userChatId": chat_id,
                "personId": cs.get("personId"),  # personId ì¶”ê°€
                "A-1": cs.get("A-1"),
                "A-2": cs.get("A-2"),
                "comment_3": cs.get("comment_3"),
                "A-4": cs.get("A-4"),
                "A-5": cs.get("A-5"),
                "comment_6": cs.get("comment_6"),
                "csatSubmittedAt": cs.get("csatSubmittedAt")
            })
            processed_count += 1
        except Exception as e:
            print(f"[CSAT] chatId={chat_id} íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

    if not rows:
        print("[CSAT] íŒŒì‹±ëœ CSAT ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    print(f"[CSAT] ì´ {len(rows)}ê°œì˜ CSAT ì‘ë‹µ íŒŒì‹± ì™„ë£Œ")

    # 6) ì›”ë³„ë¡œ ìª¼ê°œì„œ ì €ì¥ (userchats ìºì‹œì™€ ë™ì¼ ì •ì±…)
    csat_df = pd.DataFrame(rows)
    
    # CSAT ìºì‹œ ë ˆì½”ë“œ ì»¬ëŸ¼ ë³´ì¥(ë””ë²„ê·¸ í¸ì˜)
    need = ["firstAskedAt","userId","userChatId","comment_3","comment_6","A-1","A-2","A-4","A-5","csatSubmittedAt","personId"]
    for c in need:
        if c not in csat_df.columns:
            csat_df[c] = None
    
    csat_df["firstAskedAt"] = pd.to_datetime(csat_df["firstAskedAt"], errors="coerce")

    # 7) ì›”ë³„ë¡œ ìª¼ê°œì„œ ì €ì¥
    csat_df["month"] = csat_df["firstAskedAt"].dt.to_period("M").astype(str)
    total_saved = 0
    for month, mdf in csat_df.groupby("month"):
        mdf = mdf.drop(columns=["month"])
        key = f"csat_{month}"
        meta = {"month": month, "range": [start_date, end_date], "api_fetch": True, "kind": "csat"}
        ok = server_cache.save_data(key, mdf, meta)
        if ok:
            total_saved += len(mdf)
            print(f"[CSAT] {month} ì €ì¥ ì™„ë£Œ: {len(mdf)} rows")
    
    print(f"[CSAT] ì´ {total_saved} rows ì €ì¥ ì™„ë£Œ")
    return total_saved

# === ì‹ ê·œ: csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œ (triggerId í•„í„°ë§) ===
def load_csat_raw_data() -> Optional[pd.DataFrame]:
    """csat_raw.pklì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. triggerId: 768201ë§Œ ìœ íš¨."""
    try:
        df, meta = server_cache.load_data("csat_raw")
        if df is None or df.empty:
            print(f"[CSAT] csat_raw.pkl ë°ì´í„° ì—†ìŒ")
            return None
        
        # triggerId ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if "triggerId" in df.columns:
            # triggerIdê°€ 768201ì¸ ë°ì´í„°ë§Œ í•„í„°ë§
            filtered_df = df[df["triggerId"] == "768201"].copy()
            print(f"[CSAT] csat_raw.pkl ë¡œë“œ ì„±ê³µ: {len(df)} rows â†’ triggerId 768201 í•„í„°ë§: {len(filtered_df)} rows")
            return filtered_df
        else:
            # triggerId ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            print(f"[CSAT] csat_raw.pkl ë¡œë“œ ì„±ê³µ: {len(df)} rows (triggerId ì»¬ëŸ¼ ì—†ìŒ)")
            return df
            
    except Exception as e:
        print(f"[CSAT] csat_raw.pkl ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# === ì‹ ê·œ: ìºì‹œ ë¡œë”(ìºì‹œ ì „ìš©) - ìˆ˜ì •ë¨ ===
def load_csat_rows_from_cache(start_date: str, end_date: str) -> pd.DataFrame:
    """
    CSAT ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    1. ë¨¼ì € csat_YYYY-MM.pkl íŒŒì¼ë“¤ì„ ì°¾ì•„ë´…ë‹ˆë‹¤
    2. ì—†ìœ¼ë©´ csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤
    """
    def _months(s, e):
        sm = pd.to_datetime(s).to_period('M')
        em = pd.to_datetime(e).to_period('M')
        cur = sm
        out = []
        while cur <= em:
            out.append(str(cur))
            cur += 1
        return out

    # 1ë‹¨ê³„: ì›”ë³„ ìºì‹œ íŒŒì¼ë“¤ì—ì„œ ë¡œë“œ ì‹œë„
    months = _months(start_date, end_date)
    frames = []
    for month in months:
        key = f"csat_{month}"
        df, _ = server_cache.load_data(key)
        if df is not None and not df.empty:
            frames.append(df)
    
    if frames:
        # ì›”ë³„ ìºì‹œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        print(f"[CSAT] ì›”ë³„ ìºì‹œì—ì„œ {len(frames)}ê°œ íŒŒì¼ ë¡œë“œ")
        out = pd.concat(frames, ignore_index=True)
    else:
        # 2ë‹¨ê³„: csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œ
        print(f"[CSAT] ì›”ë³„ ìºì‹œ ì—†ìŒ, csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œ")
        raw_df = load_csat_raw_data()
        if raw_df is None or raw_df.empty:
            return pd.DataFrame()
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
        date_col = None
        for col in raw_df.columns:
            if any(keyword in col.lower() for keyword in ['date', 'ë‚ ì§œ', 'created', 'ìƒì„±', 'firstasked']):
                date_col = col
                break
        
        if date_col is None:
            print(f"[CSAT] ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {list(raw_df.columns)}")
            return pd.DataFrame()
        
        # ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
        try:
            raw_df[date_col] = pd.to_datetime(raw_df[date_col], errors='coerce')
            s = pd.to_datetime(start_date)
            e = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)
            out = raw_df[(raw_df[date_col].notna()) & (raw_df[date_col] >= s) & (raw_df[date_col] <= e)].copy()
            print(f"[CSAT] csat_raw.pklì—ì„œ {len(out)} rows í•„í„°ë§ ì™„ë£Œ")
        except Exception as e:
            print(f"[CSAT] ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    # ê¸°ê°„ í•„í„° (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬í–ˆì§€ë§Œ ì•ˆì „ì¥ì¹˜)
    if 'firstAskedAt' in out.columns:
        out["firstAskedAt"] = pd.to_datetime(out["firstAskedAt"], errors="coerce")
        s = pd.to_datetime(start_date)
        e = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)
        out = out[(out["firstAskedAt"].notna()) & (out["firstAskedAt"] >= s) & (out["firstAskedAt"] <= e)].reset_index(drop=True)
    
    return out

def get_filtered_df(df: pd.DataFrame, ê³ ê°ìœ í˜•="ì „ì²´", ê³ ê°ìœ í˜•_2ì°¨="ì „ì²´", ë¬¸ì˜ìœ í˜•="ì „ì²´", 
                   ë¬¸ì˜ìœ í˜•_2ì°¨="ì „ì²´", ì„œë¹„ìŠ¤ìœ í˜•="ì „ì²´", ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨="ì „ì²´") -> pd.DataFrame:
    temp = df.copy()
    required_columns = ["ê³ ê°ìœ í˜•","ê³ ê°ìœ í˜•_2ì°¨","ë¬¸ì˜ìœ í˜•","ë¬¸ì˜ìœ í˜•_2ì°¨","ì„œë¹„ìŠ¤ìœ í˜•","ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨"]
    for col in required_columns:
        if col not in temp.columns:
            temp[col] = None
    
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‹¤ì¤‘ ì„ íƒ ì²˜ë¦¬ (OR ì¡°ê±´)
    if ê³ ê°ìœ í˜• != "ì „ì²´": 
        ê³ ê°ìœ í˜•_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ê³ ê°ìœ í˜•.split(',') if v.strip()]
        if ê³ ê°ìœ í˜•_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ê³ ê°ìœ í˜•"].isin(ê³ ê°ìœ í˜•_ë¦¬ìŠ¤íŠ¸)]
    
    if ê³ ê°ìœ í˜•_2ì°¨ != "ì „ì²´": 
        ê³ ê°ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ê³ ê°ìœ í˜•_2ì°¨.split(',') if v.strip()]
        if ê³ ê°ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ê³ ê°ìœ í˜•_2ì°¨"].isin(ê³ ê°ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸)]
    
    if ë¬¸ì˜ìœ í˜• != "ì „ì²´": 
        ë¬¸ì˜ìœ í˜•_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ë¬¸ì˜ìœ í˜•.split(',') if v.strip()]
        if ë¬¸ì˜ìœ í˜•_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ë¬¸ì˜ìœ í˜•"].isin(ë¬¸ì˜ìœ í˜•_ë¦¬ìŠ¤íŠ¸)]
    
    if ë¬¸ì˜ìœ í˜•_2ì°¨ != "ì „ì²´": 
        ë¬¸ì˜ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ë¬¸ì˜ìœ í˜•_2ì°¨.split(',') if v.strip()]
        if ë¬¸ì˜ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ë¬¸ì˜ìœ í˜•_2ì°¨"].isin(ë¬¸ì˜ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸)]
    
    if ì„œë¹„ìŠ¤ìœ í˜• != "ì „ì²´": 
        ì„œë¹„ìŠ¤ìœ í˜•_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ì„œë¹„ìŠ¤ìœ í˜•.split(',') if v.strip()]
        if ì„œë¹„ìŠ¤ìœ í˜•_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ì„œë¹„ìŠ¤ìœ í˜•"].isin(ì„œë¹„ìŠ¤ìœ í˜•_ë¦¬ìŠ¤íŠ¸)]
    
    if ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ != "ì „ì²´": 
        ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨.split(',') if v.strip()]
        if ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨"].isin(ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸)]
    required_keys = ["userId","mediumType","workflowId","tags","firstAskedAt"]
    for k in required_keys:
        if k not in temp.columns:
            temp[k] = None
    for k in ["operationWaitingTime","operationAvgReplyTime","operationTotalReplyTime","operationResolutionTime"]:
        if k in temp.columns:
            temp[k] = temp[k].apply(lambda x: x if isinstance(x, str) else None)
    return temp.reset_index(drop=True)

def enrich_csat_with_user_types(csat_df: pd.DataFrame, chats_df: pd.DataFrame) -> pd.DataFrame:
    """
    csat_df:  personId, A-1, A-2, ...
    chats_df: userId, ë¬¸ì˜ìœ í˜•, ë¬¸ì˜ìœ í˜•_2ì°¨, ê³ ê°ìœ í˜•, ...
    ë°˜í™˜:     userId, ìœ í˜• ì»¬ëŸ¼ + CSAT ë¬¸í•­ì´ í•©ì³ì§„ DF
    """
    try:
        if csat_df is None or chats_df is None:
            print("[CSAT] ì…ë ¥ ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤.")
            return pd.DataFrame()
        
        if csat_df.empty or chats_df.empty:
            print("[CSAT] ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        # userId í•„ìˆ˜ ë°©ì–´ (CSAT ë°ì´í„°)
        if "userId" not in csat_df.columns:
            raise ValueError("csat_dfì— userIdê°€ ì—†ìŠµë‹ˆë‹¤.")
        if "userId" not in chats_df.columns:
            raise ValueError("chats_dfì— userIdê°€ ì—†ìŠµë‹ˆë‹¤.")

        print(f"[CSAT] CSAT ë°ì´í„° {len(csat_df)}ê±´ì— ìœ í˜• ì •ë³´ ì¶”ê°€ ì‹œì‘...")
        
        # CSAT ë°ì´í„°ëŠ” ì´ë¯¸ userIdê°€ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        csat_df_copy = csat_df.copy()
        
        # ì¡°ì¸ì— í•„ìš”í•œ ìµœì†Œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (1ì°¨ ë¶„ë¥˜ë§Œ ì‚¬ìš©)
        need_cols = ["userId", "ë¬¸ì˜ìœ í˜•", "ê³ ê°ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•"]
        use_cols = [c for c in need_cols if c in chats_df.columns]
        
        # userIdë¡œ ì´ë„ˆì¡°ì¸
        merged = pd.merge(
            csat_df_copy,
            chats_df[use_cols].drop_duplicates(subset=["userId"], keep="last"),
            on="userId",
            how="inner",
        )

        # ì¤‘ë³µ CSAT ì‘ë‹µ ì •ë¦¬: ê°™ì€ userIdì— ëŒ€í•´ csatSubmittedAt ìµœì‹  1ê±´ë§Œ ì‚¬ìš©
        if "csatSubmittedAt" in merged.columns:
            merged = (merged
                      .sort_values(["userId", "csatSubmittedAt"])
                      .drop_duplicates(subset=["userId"], keep="last"))
        
        # ë§¤ì¹­ëœ ê±´ìˆ˜ í™•ì¸
        matched_count = len(merged)
        print(f"[CSAT] ìœ í˜• ì •ë³´ ë§¤ì¹­ ì™„ë£Œ: {matched_count}/{len(csat_df)}ê±´ ({matched_count/len(csat_df)*100:.1f}%)")
        
        return merged
        
    except Exception as e:
        print(f"[CSAT] ìœ í˜• ì •ë³´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def safe_mean(series):
    """ì•ˆì „í•œ í‰ê·  ê³„ì‚° í•¨ìˆ˜ - NaN/inf ê°’ ë°©ì§€"""
    try:
        if series is None or series.empty:
            return 0.0
        non_null = series.dropna()
        if len(non_null) == 0:
            return 0.0
        mean_val = non_null.mean()
        if pd.notna(mean_val) and np.isfinite(mean_val):
            return float(mean_val)
        else:
            return 0.0
    except:
        return 0.0

def build_csat_type_scores(enriched_df: pd.DataFrame):
    """
    enriched_df: enrich_csat_with_user_types() ê²°ê³¼
      í¬í•¨ ì»¬ëŸ¼: personId, userId, ë¬¸ì˜ìœ í˜•, ë¬¸ì˜ìœ í˜•_2ì°¨, ê³ ê°ìœ í˜•, A-1, A-2, ...
    ë°˜í™˜: {
      "ë¬¸ì˜ìœ í˜•": { "A-1": [ { "ë¬¸ì˜ìœ í˜•": "...", "í‰ê· ì ìˆ˜": 4.7, "ì‘ë‹µììˆ˜": 12, "userIds": [...] }, ... ], ... },
      "ê³ ê°ìœ í˜•": { ... }
    }
    """
    import pandas as pd
    import numpy as np
    
    if enriched_df is None or enriched_df.empty:
        print(f"[CSAT] enriched_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {}

    print(f"[CSAT] ìœ í˜•ë³„ ì§‘ê³„ ì‹œì‘: {len(enriched_df)}ê±´")
    
    csat_cols = [c for c in enriched_df.columns if c.startswith("A-")]
    print(f"  - CSAT ë¬¸í•­ ì»¬ëŸ¼: {csat_cols}")
    
    result = {}

    def _group_payload(df, label_col, score_col):
        # íˆ´íŒìš© ê³„ì‚° ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        def calculate_group_averages(df, label_col, score_col):
            result = {}
            for label_val in df[label_col].unique():
                series = pd.to_numeric(df[df[label_col] == label_val][score_col], errors='coerce')
                valid = series.dropna()
                if len(valid) > 0:
                    avg = valid.mean()
                    if pd.notna(avg) and np.isfinite(avg):
                        result[label_val] = float(avg)
                    else:
                        result[label_val] = 0.0
                else:
                    result[label_val] = 0.0
            return result

        # ì‘ë‹µììˆ˜ ì§‘ê³„ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        tmp = df.copy()
        tmp[score_col] = pd.to_numeric(tmp[score_col], errors="coerce")
        tmp = tmp.dropna(subset=[label_col, score_col])
        if tmp.empty:
            return []
        
        # ì§‘ê³„: ì‘ë‹µììˆ˜ + userIds(ê³ ìœ )
        g = (tmp.groupby(label_col)
                .agg(ì‘ë‹µììˆ˜=(score_col, "count"),
                     userIds=("userId", lambda x: sorted(set(x))))
                .reset_index())
        
        # ì „ì²´ ë°ì´í„°ì—ì„œ ê³„ì‚°í•œ í‰ê· ì ìˆ˜ ë§¤í•‘
        averages = calculate_group_averages(df, label_col, score_col)
        for idx, row in g.iterrows():
            g.loc[idx, 'í‰ê· ì ìˆ˜'] = averages.get(row[label_col], 0.0)
        
        # ë””ë²„ê¹…: í‰ê· ì ìˆ˜ ê³„ì‚° ê³¼ì • í™•ì¸
        for idx, row in g.iterrows():
            label_val = row[label_col]
            print(f"  - {label_col}={label_val}: {row['ì‘ë‹µììˆ˜']}ê°œ ë°ì´í„°, í‰ê· ì ìˆ˜={row['í‰ê· ì ìˆ˜']}")
            if label_val in averages:
                print(f"    ì „ì²´ ë°ì´í„°ì—ì„œ ê³„ì‚°ëœ í‰ê· : {averages[label_val]}")
        
        # ê³¼ë„í•œ payload ë°©ì§€: userIdsëŠ” ìµœëŒ€ 50ê°œë§Œ ì œê³µ(í•„ìš”ì‹œ í™•ëŒ€)
        g["userIds"] = g["userIds"].apply(lambda li: li[:50])
        # ë§‰ëŒ€ ì°¨íŠ¸ì—ëŠ” ì‘ë‹µììˆ˜ë§Œ í‘œì‹œ, í‰ê· ì ìˆ˜ëŠ” íˆ´íŒìš©ìœ¼ë¡œë§Œ ìœ ì§€
        g["ë§‰ëŒ€ê°’"] = g["ì‘ë‹µììˆ˜"]  # ë§‰ëŒ€ëŠ” ì‘ë‹µììˆ˜ë¡œ í‘œì‹œ
        # ê²°ê³¼ dict
        records = g.sort_values("ì‘ë‹µììˆ˜", ascending=False).to_dict(orient="records")
        return records

    for label in ["ë¬¸ì˜ìœ í˜•", "ê³ ê°ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•"]:
        result[label] = {}
        for a in csat_cols:
            result[label][a] = _group_payload(enriched_df, label, a)
            print(f"  - {label}ë³„ {a}: {len(result[label][a])}ê°œ ê·¸ë£¹")

    print(f"[CSAT] ìœ í˜•ë³„ ì§‘ê³„ ì™„ë£Œ: {len(result)}ê°œ ìœ í˜•")
    return result