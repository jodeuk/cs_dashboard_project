import os
import httpx
import pandas as pd
import numpy as np
import json
import pickle
from datetime import datetime, timedelta, timezone, time as dtime
from typing import List, Dict, Optional, Tuple
import asyncio
import re
from dotenv import load_dotenv

# CSAT ë¹Œë” ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ ë½
csat_build_lock = asyncio.Lock()

print("====[CS_UTILS.PY ì½”ë“œê°€ Dockerì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤]====")

CACHE_EXPIRE_HOURS = 24

def get_cache_directory():
    is_docker = os.getenv('DOCKER_ENV') or os.path.exists('/.dockerenv')
    is_render = os.getenv('RENDER') or os.path.exists('/opt/render')
    
    if is_docker or is_render:
        cache_dir = os.getenv('CACHE_DIR', '/data/cache')
        print(f"[DEBUG] Docker/Render í™˜ê²½ ê°ì§€ë¨ - Persistent Disk ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    else:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        cache_dir = os.path.join(project_root, 'cache')
        print(f"[DEBUG] ë¡œì»¬ í™˜ê²½ - ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir, exist_ok=True)
        print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {cache_dir}")
    
    return cache_dir

CACHE_DIR = get_cache_directory()
print(f"[DEBUG] ìµœì¢… ì„¤ì •ëœ ìºì‹œ ë””ë ‰í† ë¦¬: {CACHE_DIR}")
print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼: {os.listdir(CACHE_DIR) if os.path.exists(CACHE_DIR) else 'ë””ë ‰í† ë¦¬ ì—†ìŒ'}")

load_dotenv()

# ==== ì˜ì—…ì‹œê°„ ì„¤ì • (KST, í‰ì¼ë§Œ) ====
BIZ_TZ = "Asia/Seoul"

# ì—¬ëŸ¬ êµ¬ê°„ ì§€ì›: "HH:MM-HH:MM,HH:MM-HH:MM" í˜•ì‹
# ê¸°ë³¸ê°’: í‰ì¼ 10:00-12:00, 13:00-18:00 (ì ì‹¬ 12-13 ì œì™¸)
BUSINESS_WINDOWS = os.getenv("BUSINESS_WINDOWS", "10:00-12:00,13:00-18:00")

def _parse_windows(s: str):
    wins = []
    for seg in (s or "").split(","):
        seg = seg.strip()
        if not seg or "-" not in seg:
            continue
        a, b = seg.split("-")
        ha, ma = map(int, a.split(":"))
        hb, mb = map(int, b.split(":"))
        wins.append(((ha, ma), (hb, mb)))
    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „ ê¸°ë³¸ê°’(10-12,13-18)
    return wins or [((10,0),(12,0)), ((13,0),(18,0))]

_BIZ_WINDOWS = _parse_windows(BUSINESS_WINDOWS)

# ì£¼ë§ ì œì™¸(ì›”=0 â€¦ ê¸ˆ=4)
WEEKDAYS = {0,1,2,3,4}

# CSV "YYYY-MM-DD,YYYY-MM-DD"
_HOLS = set()
for _d in filter(None, (os.getenv("BUSINESS_HOLIDAYS", "").split(","))):
    try:
        _HOLS.add(pd.to_datetime(_d.strip()).date())
    except Exception:
        pass

def _to_kst_naive(dt):
    if dt is None or pd.isna(dt):
        return None
    dt = pd.to_datetime(dt, errors="coerce")
    if pd.isna(dt):
        return None
    # tz-aware -> Asia/Seoulë¡œ ë³€í™˜ í›„ naive
    if getattr(dt, "tzinfo", None) is not None or getattr(getattr(dt, "tz", None), "zone", None):
        return dt.tz_convert(BIZ_TZ).tz_localize(None)
    return dt

def business_seconds_between(start, end,
                             windows=_BIZ_WINDOWS,
                             weekdays=WEEKDAYS,
                             holidays=_HOLS):
    """
    start~end ì‚¬ì´ì—ì„œ 'í‰ì¼'ì˜ ì§€ì •ëœ ì˜ì—…ì‹œê°„ êµ¬ê°„ë“¤ë§Œ ëˆ„ì (ì´ˆ).
    ì˜ˆ: windows=[((10,0),(12,0)), ((13,0),(18,0))]  â†’ ì ì‹¬ ì‹œê°„ ì œì™¸
    """
    s = _to_kst_naive(start); e = _to_kst_naive(end)
    if s is None or e is None or e <= s:
        return 0

    total = 0
    cur = s.normalize()  # 00:00
    end_day = e.normalize()
    while cur <= end_day:
        d = cur.date()
        if (cur.weekday() in weekdays) and (d not in holidays):
            for (sh, sm), (eh, em) in windows:
                day_start = cur.replace(hour=sh, minute=sm, second=0, microsecond=0)
                day_end   = cur.replace(hour=eh, minute=em, second=0, microsecond=0)
                # ì´ êµ¬ê°„ê³¼ [s,e] êµì§‘í•©
                seg_start = max(s, day_start)
                seg_end   = min(e, day_end)
                if seg_end > seg_start:
                    total += int((seg_end - seg_start).total_seconds())
        cur += pd.Timedelta(days=1)
    return max(0, total)

def seconds_to_hms(sec: int) -> str:
    if sec <= 0:
        return "00:00:00"
    h = sec // 3600
    m = (sec % 3600) // 60
    s = sec % 60
    return f"{h:02}:{m:02}:{s:02}"

# === [NEW] ì˜ì—…ì‹œê°„ ê³„ì‚° ìœ í‹¸ ==============================
WORK_BLOCKS = [(dtime(10,0), dtime(12,0)), (dtime(13,0), dtime(18,0))]  # í‰ì¼ 10-12, 13-18
WORKWEEK = set(range(0,5))  # ì›”(0)~ê¸ˆ(4)

def _overlap_minutes(a_start, a_end, b_start, b_end) -> int:
    s = max(a_start, b_start)
    e = min(a_end, b_end)
    if e <= s:
        return 0
    return int((e - s).total_seconds() // 60)

def working_minutes_between_kst(start_dt, end_dt) -> int:
    """
    start_dt ~ end_dt ì‚¬ì´ì˜ 'ì˜ì—…ì‹œê°„' ë¶„(min)ë§Œ ëˆ„ì .
    - í‰ì¼ë§Œ ì¹´ìš´íŠ¸
    - 10:00~12:00, 13:00~18:00
    - ì ì‹¬(12~13)ì€ ì œì™¸
    - ì…ë ¥ì€ KST 'naive' Timestampë¥¼ ê°€ì • (convert_timeì—ì„œ ë³´ì •)
    """
    try:
        if pd.isna(start_dt) or pd.isna(end_dt) or end_dt <= start_dt:
            return 0
        # ì¼ ë‹¨ìœ„ ë£¨í”„
        minutes = 0
        cur = pd.Timestamp(start_dt.date())
        last = pd.Timestamp(end_dt.date())
        while cur <= last:
            if cur.weekday() in WORKWEEK:
                for (ws, we) in WORK_BLOCKS:
                    w_start = pd.Timestamp.combine(cur, ws)
                    w_end   = pd.Timestamp.combine(cur, we)
                    minutes += _overlap_minutes(start_dt, end_dt, w_start, w_end)
            cur += pd.Timedelta(days=1)
        return minutes
    except Exception:
        return 0

def _fmt_hhmmss_from_minutes(mins: int) -> str:
    h = mins // 60
    m = mins % 60
    return f"{int(h):02}:{int(m):02}:00"

# === [REPLACE] attach_resolution_fallback ==================
def attach_resolution_fallback(df: pd.DataFrame) -> pd.DataFrame:
    """
    operationResolutionTime(ë¬¸ìì—´ H:M:S) ì´ ë¹„ì–´ìˆëŠ” í–‰ì— í•œí•´ì„œ,
    openedAt~closedAt ì˜ì—…ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°í•œ ê°’ì„ ì§ì ‘ ì±„ì›Œë„£ëŠ”ë‹¤.
    - ì ì‹¬(12~13) ì œì™¸, í‰ì¼ 10-12/13-18ë§Œ ì¹´ìš´íŠ¸
    - ê³„ì‚° ê²°ê³¼ê°€ 0ë¶„ì´ë©´ ì±„ìš°ì§€ ì•ŠìŒ(ê·¸ëŒ€ë¡œ None ìœ ì§€)
    """
    if df is None or df.empty:
        return df

    out = df.copy()
    for c in ["openedAt", "closedAt"]:
        if c not in out.columns:
            out[c] = pd.NaT
        else:
            out[c] = pd.to_datetime(out[c], errors="coerce")

    if "operationResolutionTime" not in out.columns:
        out["operationResolutionTime"] = None

    def _blank_time(x):
        if x is None: return True
        if isinstance(x, str):
            s = x.strip().lower()
            return s in ("", "nan", "null", "undefined", "00:00:00")
        return False

    # í•„ìš” í–‰ë§Œ ê³„ì‚°í•´ì„œ ë®ì–´ì“°ê¸°
    for idx, row in out.iterrows():
        base = row.get("operationResolutionTime")
        if not _blank_time(base):
            continue  # ì´ë¯¸ ê°’ ìˆìœ¼ë©´ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
        oa, ca = row.get("openedAt"), row.get("closedAt")
        if pd.isna(oa) or pd.isna(ca):
            continue
        mins = working_minutes_between_kst(oa, ca)
        if mins and mins > 0:
            out.at[idx, "operationResolutionTime"] = _fmt_hhmmss_from_minutes(mins)

    return out
# ===========================================================

class ChannelTalkAPI:
    def __init__(self):
        self.base_url = "https://api.channel.io"
        self.access_key = os.getenv("CHANNEL_ACCESS_KEY")
        self.access_secret = os.getenv("CHANNEL_ACCESS_SECRET")
        self.headers = {
            "x-access-key": self.access_key,
            "x-access-secret": self.access_secret,
            "Content-Type": "application/json"
        }

    # === (RESTORED) CSAT ë¼ë²¨ ì •ê·œí™” ìœ í‹¸ ===
    def _clean_label(self, s: str) -> str:
        """í¼ ë¼ë²¨ì˜ ì•ë’¤ ê³µë°±/ë¶ˆí•„ìš”í•œ êµ¬ë¶„ì ì œê±° (í•œê¸€ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ì¡´)."""
        if not s:
            return ""
        s = str(s)
        # ì¤„ë°”ê¿ˆ/ì¤‘ë³µ ê³µë°± ì •ë¦¬ ë° í”í•œ êµ¬ë¶„ì ì œê±°
        s = re.sub(r"\s+", " ", s).strip()
        s = s.replace("Â·", " ").replace("â€¢", " ").replace(":", " ").replace("â€“", "-").replace("â€”", "-")
        return s

    def _leading_num(self, s: str):
        """ë¬¸í•­ ì•ì˜ ìˆ«ì(ì˜ˆ: '1) ...', '3 . ...')ë¥¼ ì •ìˆ˜ë¡œ ë°˜í™˜. ì—†ìœ¼ë©´ None."""
        if not s:
            return None
        m = re.match(r"^\s*([0-9]+)", str(s))
        return int(m.group(1)) if m else None

    async def _ensure_keys(self):
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    async def get_all_users(self, limit: int = 100) -> List[Dict]:
        await self._ensure_keys()
        url = f"{self.base_url}/open/v5/users"
        params = {"limit": limit}
        async with httpx.AsyncClient(timeout=30.0) as client:
            r = await client.get(url, headers=self.headers, params=params)
            r.raise_for_status()
            return r.json().get("users", [])

    async def get_userchats(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        await self._ensure_keys()
        start_ts = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_ts = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)

        all_userchats, since, page_count = [], None, 0
        max_pages = 10
        collected_ids = set()  # userChat ë‹¨ìœ„ë¡œ ì¤‘ë³µ ë°©ì§€
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/user-chats"
                params = {"limit": limit, "state": "closed"}
                if since:
                    params["since"] = since
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    resp = await client.get(url, headers=self.headers, params=params)
                    resp.raise_for_status()
                    data = resp.json()

                user_chats = data.get("userChats", [])
                next_value = data.get("next")
                
                if not user_chats:
                    break
                
                for chat in user_chats:
                    chat_id = (
                        chat.get("id")
                        or chat.get("chatId")
                        or (chat.get("mainKey") or "").replace("userChat-", "")
                    )
                    if chat_id and chat_id not in collected_ids:
                        all_userchats.append(chat)
                        collected_ids.add(chat_id)
                
                if not next_value or not str(next_value).strip():
                    break
                
                since = next_value
                    
        except Exception as e:
            print(f"[get_userchats] ì˜¤ë¥˜: {e}")
            raise
        
        print(f"[API] ì´ ìˆ˜ì§‘ëœ ì±„íŒ… ìˆ˜: {len(all_userchats)} (ê¸°ê°„: {start_date} ~ {end_date})")
        await self._hydrate_open_closed(all_userchats)  # openedAt/closedAt ë³´ê°• (update/refresh ê²½ë¡œì—ì„œë§Œ)
        print(f"[API] openedAt/closedAt ë³´ê°• ì™„ë£Œ í›„ ë°˜í™˜: {len(all_userchats)} chats")
        return all_userchats

    async def get_userchat_by_id(self, userchat_id: str) -> Dict:
        await self._ensure_keys()
        url = f"{self.base_url}/open/v5/user-chats/{userchat_id}"
        async with httpx.AsyncClient(timeout=30.0) as client:
            r = await client.get(url, headers=self.headers)
            r.raise_for_status()
            return r.json()

    async def _hydrate_open_closed(self, chats: list, max_concurrency: int = 8) -> None:
        """
        openedAt/closedAtê°€ ë¹„ì–´ìˆëŠ” userChatë§Œ ìƒì„¸ API(/open/v5/user-chats/{id})ë¡œ ë³´ê°•.
        chats ë¦¬ìŠ¤íŠ¸ ì›ì†Œë¥¼ in-place ìˆ˜ì •.
        """
        import asyncio
        sem = asyncio.Semaphore(max_concurrency)

        async def fetch_and_patch(chat):
            # ì´ë¯¸ ë‘˜ ë‹¤ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if chat.get("openedAt") is not None and chat.get("closedAt") is not None:
                return
            chat_id = chat.get("id") or chat.get("chatId") or chat.get("mainKey", "").replace("userChat-", "")
            if not chat_id:
                return
            async with sem:
                try:
                    detail = await self.get_userchat_by_id(str(chat_id))
                    # â†“ ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ í‚¤ í›„ë³´ ëª‡ ê°œ ëŒ€ë¹„
                    opened = (detail.get("openedAt")
                              or detail.get("openAt")
                              or (detail.get("state") or {}).get("openedAt"))
                    closed = (detail.get("closedAt")
                              or detail.get("closeAt")
                              or (detail.get("state") or {}).get("closedAt"))
                    created = detail.get("createdAt", chat.get("createdAt"))

                    if opened is not None: chat["openedAt"] = opened
                    if closed is not None: chat["closedAt"] = closed
                    if created is not None: chat["createdAt"] = created
                except Exception as e:
                    print(f"[HYDRATE] chatId={chat_id} ìƒì„¸ì¡°íšŒ ì‹¤íŒ¨: {e}")

        targets = [c for c in chats if c.get("openedAt") is None or c.get("closedAt") is None]
        if not targets:
            return

        print(f"[HYDRATE] openedAt/closedAt ë³´ê°• ëŒ€ìƒ: {len(targets)}ê°œ (ì „ì²´ {len(chats)}ê°œ)")
        await asyncio.gather(*(fetch_and_patch(c) for c in targets))

    async def get_messages(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        await self._ensure_keys()
        start_ts = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_ts = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)

        all_messages, since, page_count = [], None, 0
        max_pages = 10
        collected_ids = set()
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/messages"
                params = {"limit": limit}
                if since:
                    params["since"] = since

                async with httpx.AsyncClient(timeout=30.0) as client:
                    r = await client.get(url, headers=self.headers, params=params)
                    r.raise_for_status()
                    data = r.json()
                    messages = data.get("messages", [])
                    next_value = data.get("next")

                    if not messages:
                        break

                    filtered = []
                    for m in messages:
                        mid = m.get("id")
                        if mid not in collected_ids:
                            created = m.get("createdAt")
                            if created and start_ts <= created <= end_ts:
                                filtered.append(m)
                                collected_ids.add(mid)
                    all_messages.extend(filtered)

                    if not next_value or not str(next_value).strip():
                        break
                    since = next_value
        except Exception as e:
            print(f"[get_messages] ì˜¤ë¥˜: {e}")
            raise

        print(f"[MESSAGES API] ì´ ìˆ˜ì§‘ëœ ë©”ì‹œì§€ ìˆ˜: {len(all_messages)} (ê¸°ê°„: {start_date} ~ {end_date})")
        return all_messages

    # â–¼ ì‹ ê·œ: íŠ¹ì • userChatì˜ ë©”ì‹œì§€ ì „ë¶€(pagination) ì¡°íšŒ
    async def get_messages_by_chat(self, user_chat_id: str, limit: int = 500, sort_order: str = "desc") -> List[Dict]:
        await self._ensure_keys()
        url = f"{self.base_url}/open/v5/user-chats/{user_chat_id}/messages"
        since = None
        out: List[Dict] = []
        guard = 0
        try:
            while True:
                params = {"limit": limit, "sortOrder": sort_order}
                if since:
                    params["since"] = since
                async with httpx.AsyncClient(timeout=30.0) as client:
                    r = await client.get(url, headers=self.headers, params=params)
                    r.raise_for_status()
                    data = r.json()
                    msgs = data.get("messages", [])
                    next_value = data.get("next")
                    out.extend(msgs)
                    if not next_value:
                        break
                    since = next_value
                    guard += 1
                    if guard > 50:
                        break
        except Exception as e:
            print(f"[get_messages_by_chat] ì˜¤ë¥˜: chatId={user_chat_id}, {e}")
            raise
        return out

    def hms_to_seconds(self, time_str: str) -> int:
        try:
            hours, minutes, seconds = map(int, time_str.split(':'))
            return hours * 3600 + minutes * 60 + seconds
        except:
            return 0

    def extract_level(self, tags: List[str], type_name: str, level: int) -> Optional[str]:
        if not tags:
            return None
        for tag in tags:
            if tag.startswith(f"{type_name}/"):
                parts = tag.split("/")
                if len(parts) > level:
                    # level 1 = parts[1], level 2 = parts[2] (0ë²ˆì§¸ëŠ” íƒ€ì…ëª…)
                    return parts[level]
        return None

    async def process_userchat_data(self, data: List[Dict]) -> pd.DataFrame:
        # (êµì²´) ì¶”ì¶œ í‚¤ ëª©ë¡
        keep_keys = [
            "userId", "personId", "mediumType", "workflowId", "tags", "chats",
            "createdAt", "firstAskedAt", "openedAt", "closedAt",
            "operationWaitingTime", "operationAvgReplyTime",
            "operationTotalReplyTime", "operationResolutionTime"
        ]

        # (êµì²´) ë³€í™˜ê¸°
        def convert_time(key, ms):
            if ms is None:
                return None
            try:
                if key in {"firstAskedAt", "createdAt", "openedAt", "closedAt"}:
                    if isinstance(ms, str):
                        return pd.to_datetime(ms, errors='coerce')
                    elif isinstance(ms, (int, float)):
                        # UTC(ms) -> KST naive
                        return pd.to_datetime(ms, unit='ms', utc=True).tz_convert('Asia/Seoul').tz_localize(None)
                    elif isinstance(ms, (pd.Timestamp, datetime)):
                        return pd.to_datetime(ms, errors='coerce')
                    else:
                        return pd.NaT

                if key in {"operationWaitingTime","operationAvgReplyTime","operationTotalReplyTime","operationResolutionTime"}:
                    td = timedelta(milliseconds=ms)
                    total_seconds = int(td.total_seconds())
                    h, m, s = total_seconds // 3600, (total_seconds % 3600) // 60, total_seconds % 60
                    return f"{h:02}:{m:02}:{s:02}"
                return None
            except Exception:
                return None

        processed_data = []
        for item in data:
            first_asked_at = item.get("firstAskedAt")
            if first_asked_at is None:
                continue
            
            tags = item.get('tags', [])
            new_obj = {}
            for key in keep_keys:
                value = item.get(key)
                if key == "workflowId":
                    value = item.get("source", {}).get("workflow", {}).get("id")
                elif key in ["firstAskedAt","createdAt","openedAt","closedAt",
                             "operationWaitingTime","operationAvgReplyTime",
                             "operationTotalReplyTime","operationResolutionTime"]:
                    value = convert_time(key, value)
                new_obj[key] = value

            # íƒœê·¸ ì²˜ë¦¬ ê³¼ì • ë¡œê¹…
            print(f"[TAGS] ì•„ì´í…œ íƒœê·¸: {tags}")

            ê³ ê°ìœ í˜•_1ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 1)
            ê³ ê°ìœ í˜•_2ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 2)
            ë¬¸ì˜ìœ í˜•_1ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 1)
            ë¬¸ì˜ìœ í˜•_2ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 2)
            ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 1)
            ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 2)
            ì²˜ë¦¬ìœ í˜•_1ì°¨ = self.extract_level(tags, "ì²˜ë¦¬ìœ í˜•", 1)
            ì²˜ë¦¬ìœ í˜•_2ì°¨ = self.extract_level(tags, "ì²˜ë¦¬ìœ í˜•", 2)

            # 1ì°¨ ë¶„ë¥˜ë§Œ ì‚¬ìš© (2ì°¨ ë¶„ë¥˜ ì œê±°)
            ê³ ê°ìœ í˜• = ê³ ê°ìœ í˜•_1ì°¨
            ë¬¸ì˜ìœ í˜• = ë¬¸ì˜ìœ í˜•_1ì°¨
            ì„œë¹„ìŠ¤ìœ í˜• = ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨
            
            print(f"[EXTRACT] ë¬¸ì˜ìœ í˜•: {ë¬¸ì˜ìœ í˜•} (1ì°¨: {ë¬¸ì˜ìœ í˜•_1ì°¨}, 2ì°¨: {ë¬¸ì˜ìœ í˜•_2ì°¨})")

            processed_item = {
                **new_obj,
                "ê³ ê°ìœ í˜•": ê³ ê°ìœ í˜•,
                "ë¬¸ì˜ìœ í˜•": ë¬¸ì˜ìœ í˜•,
                "ì„œë¹„ìŠ¤ìœ í˜•": ì„œë¹„ìŠ¤ìœ í˜•,
                "ì²˜ë¦¬ìœ í˜•": ì²˜ë¦¬ìœ í˜•_1ì°¨,
                "ê³ ê°ìœ í˜•_1ì°¨": ê³ ê°ìœ í˜•_1ì°¨,
                "ë¬¸ì˜ìœ í˜•_1ì°¨": ë¬¸ì˜ìœ í˜•_1ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨,
                "ì²˜ë¦¬ìœ í˜•_1ì°¨": ì²˜ë¦¬ìœ í˜•_1ì°¨,
                "ê³ ê°ìœ í˜•_2ì°¨": ê³ ê°ìœ í˜•_2ì°¨,
                "ë¬¸ì˜ìœ í˜•_2ì°¨": ë¬¸ì˜ìœ í˜•_2ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨,
                "ì²˜ë¦¬ìœ í˜•_2ì°¨": ì²˜ë¦¬ìœ í˜•_2ì°¨,
                # userChat id
                "userChatId": item.get("id") or item.get("chatId") or item.get("mainKey", "").replace("userChat-", "")
            }
            processed_data.append(processed_item)

        df = pd.DataFrame(processed_data)
        
        # (ì¶”ê°€) DF ìƒì„± í›„ ë³´ì •
        if "firstAskedAt" in df.columns:
            df["firstAskedAt"] = pd.to_datetime(df["firstAskedAt"], errors='coerce')
        for c in ["createdAt","openedAt","closedAt"]:
            if c in df.columns:
                df[c] = pd.to_datetime(df[c], errors='coerce')

        # (êµì²´) í•„ìˆ˜ ì»¬ëŸ¼
        required_columns = [
            "ê³ ê°ìœ í˜•","ë¬¸ì˜ìœ í˜•","ì„œë¹„ìŠ¤ìœ í˜•",
            "ê³ ê°ìœ í˜•_1ì°¨","ë¬¸ì˜ìœ í˜•_1ì°¨","ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨",
            "ê³ ê°ìœ í˜•_2ì°¨","ë¬¸ì˜ìœ í˜•_2ì°¨","ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨",
            "firstAskedAt","createdAt","openedAt","closedAt",
            "operationWaitingTime","operationAvgReplyTime",
            "operationTotalReplyTime","operationResolutionTime",
            "userChatId","userId","personId"
        ]
        for col in required_columns:
            if col not in df.columns:
                df[col] = None

        return df


    # â–¼ ì‹ ê·œ: ë©”ì‹œì§€ ë°°ì—´ì—ì„œ CSAT ì„¤ë¬¸ ì¶”ì¶œ
    def extract_csat_from_messages(self, msgs: List[Dict], allowed_trigger_ids: Optional[set] = None) -> Dict:
        """
        ...
        """
        result = {}
        latest_submit_ts = None
        person_id = None

        # â–¶ ë¶„ëª¨/ë¶„ì í”Œë˜ê·¸ ì´ˆê¸°í™”
        result.setdefault("wf_768201_started", False)  # ì„¤ë¬¸ 'ì‹œì‘ì'(ëŒ€ìƒì)
        result.setdefault("has_score_any", False)      # A-1/2/4/5 ì¤‘ í•˜ë‚˜ë¼ë„ ì‘ë‹µ

        allowed_set = set(map(str, allowed_trigger_ids or []))

        def norm_label(label: str) -> Optional[str]:
            s = self._clean_label(label)
            if not s:
                return None

            # ìˆ«ì ì ‘ë‘ ìë™ ì¸ì‹ (ì˜ˆ: "3) ..." "6 . ..." í¬í•¨)
            n = self._leading_num(s)
            if n == 1: return "A-1"
            if n == 2: return "A-2"
            if n == 3: return "comment_3"
            if n == 4: return "A-4"
            if n == 5: return "A-5"
            if n == 6: return "comment_6"

            # ë°±ì—… ê·œì¹™(í‚¤ì›Œë“œ)
            if "ì¹œì ˆë„" in s: return "A-1"
            if "ë¬¸ì œ í•´ê²°" in s: return "A-2"
            if "ìƒë‹´ ê³¼ì •" in s or "ê°œì„ ì " in s: return "comment_3"
            if "ì•ˆì •ì„±" in s: return "A-4"
            if "ë””ìì¸" in s: return "A-5"
            if "í”Œë«í¼ì— ëŒ€í•´" in s: return "comment_6"
            return None

        for m in msgs:
            log = m.get("log") or {}
            wf = (m.get("workflow") or {})
            wf_id = str(wf.get("id") or "")
            trig_type = log.get("triggerType")
            trig_id = str(log.get("triggerId")) if log.get("triggerId") is not None else ""
            action = (log.get("action") or log.get("type") or "").lower()

            # personId ì¶”ì¶œ (user ë©”ì‹œì§€ ê¸°ì¤€)
            if person_id is None and m.get("personType") == "user":
                person_id = m.get("personId") or m.get("person", {}).get("id")

            # === [í•µì‹¬] 'ì„¤ë¬¸ ì‹œì‘' ë„“ê²Œ ê°ì§€ ===
            # 1) ë¡œê·¸ íŠ¸ë¦¬ê±°: action ì´ start* ë¥˜
            if trig_type == "workflow" and (not allowed_set or trig_id in allowed_set) and action.startswith("start"):
                result["wf_768201_started"] = True
            # 2) ë©”ì‹œì§€ì— workflow.id ê°€ ëª©í‘œ ID
            if wf_id and (not allowed_set or wf_id in allowed_set):
                result["wf_768201_started"] = True

            form = m.get("form")
            inputs = (form or {}).get("inputs") or []
            # 3) CSAT ë°”ì¸ë”© í‚¤ê°€ í¬í•¨ëœ í¼ì´ë©´ 'ì‹œì‘'ìœ¼ë¡œ ê°„ì£¼ (ì ìˆ˜ ë¯¸ì‘ë‹µì´ì–´ë„ ëŒ€ìƒì)
            has_csat_input = any((inp.get("bindingKey") or "").startswith("userChat.profile.csat") for inp in inputs)
            if has_csat_input:
                result["wf_768201_started"] = True

            # --- ì œì¶œ ì‹œê° (ìˆìœ¼ë©´ ìµœì‹ ê°’ ìœ ì§€)
            submitted_at = (form or {}).get("submittedAt")
            if submitted_at:
                latest_submit_ts = max(latest_submit_ts or submitted_at, submitted_at)

            # === [ì™„í™”ëœ íŒŒì‹± ê²Œì´íŠ¸] ===
            # allowed_trigger_ids ê°€ ì£¼ì–´ì¡Œë”ë¼ë„,
            # - íŠ¸ë¦¬ê±°ID ë§¤ì¹˜ OR workflow.id ë§¤ì¹˜ OR CSAT ì…ë ¥ í¼ í¬í•¨ ì´ë©´ íŒŒì‹± í—ˆìš©
            parse_ok = True
            if allowed_set:
                parse_ok = (trig_type == "workflow" and trig_id in allowed_set) or (wf_id in allowed_set) or has_csat_input
            if not parse_ok:
                continue

            # === í¼ ì…ë ¥ â†’ ë¼ë²¨ ë§¤í•‘(ê¸°ì¡´ ë¡œì§ ìœ ì§€) ===
            if form:
                in_first_group  = wf.get("actionIndex") in (0, 1)
                in_second_group = wf.get("actionIndex") in (2, 3)

                for inp in inputs:
                    original_label = inp.get("label")
                    label = norm_label(original_label)
                    val = inp.get("value")
                    bk = (inp.get("bindingKey") or "").strip()

                    # Fallback: ì½”ë©˜íŠ¸ ë¼ë²¨ ì¶”ì •
                    if not label and bk == "userChat.profile.csat":
                        pass  # ì ìˆ˜ ë¬¸í•­ ì¶”ì •ì€ ìœ„í—˜ â†’ ìŠ¤í‚µ
                    if not label and bk == "userChat.profile.csatComment":
                        label = "comment_3" if in_first_group else "comment_6" if in_second_group else None

                    if original_label and val:
                        print(f"[CSAT_DEBUG] ì›ë³¸: '{original_label}' â†’ ë§¤í•‘: '{label}' â†’ ê°’: '{val}'")

                    if not label:
                        continue

                    if label.startswith("A-"):
                        try:
                            result[label] = int(val) if val is not None else None
                        except Exception:
                            result[label] = None
                        if label in ("A-1","A-2","A-4","A-5"):
                            if val is not None and str(val).strip() != "":
                                try:
                                    int(val)
                                    result["has_score_any"] = True
                                except Exception:
                                    pass
                    elif label.startswith("comment_"):
                        if label not in result:
                            result[label] = []
                        if isinstance(val, str) and val.strip():
                            result[label].append(val.strip())

        # ì œì¶œ ISO(KST)
        if latest_submit_ts:
            try:
                kst = timezone(timedelta(hours=9))
                dt = datetime.fromtimestamp(latest_submit_ts/1000, tz=kst)
                result["csatSubmittedAt"] = dt.isoformat()
            except Exception:
                pass

        if person_id:
            result["personId"] = person_id

        # comment ë°°ì—´ â†’ ë¬¸ìì—´
        for key in ["comment_3", "comment_6"]:
            if key in result:
                if isinstance(result[key], list):
                    result[key] = result[key][0].strip() if result[key] else None
                elif isinstance(result[key], str):
                    result[key] = result[key].strip() or None

        return result

    # â–¼ ì‹ ê·œ: CSAT ë°ì´í„°ê°€ ìˆëŠ” userChatë§Œ í•„í„°ë§
    def filter_csat_userchats(self, userchats: List[Dict]) -> List[Dict]:
        """CSAT ë°ì´í„°ê°€ ìˆëŠ” userChatë§Œ í•„í„°ë§ (workflowId: 768201)"""
        csat_chats = []
        for chat in userchats:
            # workflowId í™•ì¸
            workflow_id = chat.get("source", {}).get("workflow", {}).get("id")
            if workflow_id == "768201":
                csat_chats.append(chat)
        return csat_chats

channel_api = ChannelTalkAPI()

class ServerCache:
    def __init__(self, cache_dir=None):
        self.cache_dir = cache_dir or CACHE_DIR
        print(f"[CACHE] ServerCache ì´ˆê¸°í™” - ìºì‹œ ë””ë ‰í† ë¦¬: {os.path.abspath(self.cache_dir)}")
        self.ensure_cache_dir()
    
    def ensure_cache_dir(self):
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir, exist_ok=True)
    
    def get_cache_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def get_metadata_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}_metadata.json")
    
    def save_data(self, cache_key: str, data: pd.DataFrame, metadata: Dict):
        try:
            self.ensure_cache_dir()
            metadata.update({
                "saved_at": datetime.now().isoformat(),
                "data_count": len(data),
                "cache_version": "1.1"
            })
            if "firstAskedAt" in data.columns and not data.empty:
                data["firstAskedAt"] = pd.to_datetime(data["firstAskedAt"], errors="coerce")
                valid = data["firstAskedAt"].dropna()
                def safe_iso(x):
                    if isinstance(x, (pd.Timestamp, datetime)):
                        return x.isoformat()
                    try:
                        return pd.to_datetime(x, errors="coerce").isoformat()
                    except Exception:
                        return ""
                metadata["first_asked_start"] = safe_iso(valid.min()) if not valid.empty else ""
                metadata["first_asked_end"] = safe_iso(valid.max()) if not valid.empty else ""
            data_path = self.get_cache_path(cache_key)
            data.to_pickle(data_path)
            meta_path = self.get_metadata_path(cache_key)
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"[CACHE] ì €ì¥ ì™„ë£Œ: {cache_key} ({len(data)} rows)")
            return True
        except Exception as e:
            print(f"[CACHE] save_data ì‹¤íŒ¨: {e}")
            return False
    
    def load_data(self, cache_key: str):
        data_path = self.get_cache_path(cache_key)
        meta_path = self.get_metadata_path(cache_key)
        if os.path.exists(data_path) and os.path.exists(meta_path):
            df = pd.read_pickle(data_path)
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
            return df, meta
        return None, None

    def clear_all_cache(self) -> bool:
        try:
            for fn in os.listdir(self.cache_dir):
                if fn.endswith(".pkl") or fn.endswith("_metadata.json"):
                    os.remove(os.path.join(self.cache_dir, fn))
            return True
        except Exception as e:
            print(f"[CACHE] ì „ì²´ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def is_cache_still_valid(self, metadata: dict) -> bool:
        if not metadata or "saved_at" not in metadata:
            return False
        try:
            saved_at = pd.to_datetime(metadata["saved_at"])
            now = pd.Timestamp.now(tz=saved_at.tz if hasattr(saved_at, 'tz') else None)
            hours = (now - saved_at).total_seconds() / 3600
            return hours < CACHE_EXPIRE_HOURS
        except Exception:
            return False

    def filter_data_by_date_range(self, df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            first = pd.to_datetime(df['firstAskedAt'], errors='coerce')
            if getattr(first.dt, "tz", None) is not None:
                first = first.dt.tz_convert('Asia/Seoul').dt.tz_localize(None)

            start_kst = pd.to_datetime(start_date)
            end_kst = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)
            mask = (first >= start_kst) & (first <= end_kst)
            out = df.loc[mask].copy()
            return out
        except Exception as e:
            print(f"[CACHE] ë‚ ì§œ ë²”ìœ„ í•„í„° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
server_cache = ServerCache()

# === ìºì‹œ ë³‘í•© ìœ í‹¸ ===
def get_cached_data_month(month: str) -> Optional[pd.DataFrame]:
    cache_key = f"userchats_{month}"
    df, _ = server_cache.load_data(cache_key)
    return df

async def get_cached_data(start_date: str, end_date: str, refresh_mode: str = "cache") -> pd.DataFrame:
    """
    refresh_mode:
    - "cache": ê¸°ì¡´ ìºì‹œë§Œ ì‚¬ìš© (ê¸°ë³¸ê°’)
    - "update": ê¸°ì¡´ ìºì‹œ ìœ ì§€ + ëˆ„ë½ëœ ê¸°ê°„ë§Œ API í˜¸ì¶œ
    - "refresh": ê¸°ì¡´ ìºì‹œ ì™„ì „ ì‚­ì œ + ì „ì²´ ìƒˆë¡œ ìˆ˜ì§‘
    """
    def _months(s, e):
        sm = pd.to_datetime(s).to_period('M')
        em = pd.to_datetime(e).to_period('M')
        cur = sm
        out = []
        while cur <= em:
            out.append(str(cur))
            cur += 1
        return out

    months = _months(start_date, end_date)
    all_data = []
    
    for month in months:
        if refresh_mode == "refresh":
            # ì „ì²´ ê°±ì‹ : ê¸°ì¡´ ìºì‹œ ë¬´ì‹œí•˜ê³  API í˜¸ì¶œ
            print(f"[REFRESH] {month} â†’ userchats API í˜¸ì¶œ í›„ ì €ì¥")
            year, m = map(int, month.split('-'))
            start = datetime(year, m, 1).strftime("%Y-%m-%d")
            if m == 12:
                end = datetime(year+1, 1, 1) - timedelta(days=1)
            else:
                end = datetime(year, m+1, 1) - timedelta(days=1)
            end = end.strftime("%Y-%m-%d")
            try:
                userchats = await channel_api.get_userchats(start, end)
                if userchats:
                    df = await channel_api.process_userchat_data(userchats)
                    df = attach_resolution_fallback(df)
                    meta = {"month": month, "range": [start, end], "api_fetch": True}
                    server_cache.save_data(f"userchats_{month}", df, meta)
                    all_data.append(df)
            except Exception as e:
                print(f"[REFRESH] {month} ì‹¤íŒ¨: {e}")
        elif refresh_mode == "update":
            # ìµœì‹ í™”: ê¸°ì¡´ ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ API í˜¸ì¶œ
            df = get_cached_data_month(month)
            year, m = map(int, month.split('-'))
            current_year = datetime.now().year
            current_month = datetime.now().month
            
            if df is not None and not df.empty:
                if year == current_year and m == current_month:
                    # í˜„ì¬ ë‹¬(8ì›”): ê¸°ì¡´ ìºì‹œ + ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
                    print(f"[UPDATE] {month} í˜„ì¬ ë‹¬ â†’ ê¸°ì¡´ ìºì‹œ({len(df)} rows) + ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€")
                    start = datetime(year, m, 1).strftime("%Y-%m-%d")
                    if m == 12:
                        end = datetime(year+1, 1, 1) - timedelta(days=1)
                    else:
                        end = datetime(year, m+1, 1) - timedelta(days=1)
                    end = end.strftime("%Y-%m-%d")
                    try:
                        userchats = await channel_api.get_userchats(start, end)
                        if userchats:
                            new_df = await channel_api.process_userchat_data(userchats)
                            # ê¸°ì¡´ ë°ì´í„° + ìƒˆë¡œìš´ ë°ì´í„° í•©ì¹˜ê¸°
                            combined_df = pd.concat([df, new_df], ignore_index=True)
                            # ì¤‘ë³µ ì œê±° (userChatId ê¸°ì¤€ - ê° ë¬¸ì˜ë§ˆë‹¤ ê³ ìœ )
                            combined_df = combined_df.drop_duplicates(subset=['userChatId'], keep='first')
                            combined_df = attach_resolution_fallback(combined_df)
                            # ì—…ë°ì´íŠ¸ëœ ìºì‹œ ì €ì¥
                            meta = {"month": month, "range": [start, end], "api_fetch": True, "updated": True}
                            server_cache.save_data(f"userchats_{month}", combined_df, meta)
                            print(f"[UPDATE] {month} ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(df)} â†’ {len(combined_df)} rows")
                            all_data.append(combined_df)
                        else:
                            print(f"[UPDATE] {month} ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ, ê¸°ì¡´ ìºì‹œ ì‚¬ìš©")
                            all_data.append(df)
                    except Exception as e:
                        print(f"[UPDATE] {month} í˜„ì¬ ë‹¬ ê°±ì‹  ì‹¤íŒ¨: {e}, ê¸°ì¡´ ìºì‹œ ì‚¬ìš©")
                        all_data.append(df)
                else:
                    # ê³¼ê±° ë‹¬: ê¸°ì¡´ ìºì‹œ ì‚¬ìš©
                    print(f"[UPDATE] {month} ê³¼ê±° ë‹¬ â†’ ìºì‹œ ì‚¬ìš© ({len(df)} rows)")
                    all_data.append(df)
            else:
                # ìºì‹œê°€ ì—†ëŠ” ê²½ìš°: API í˜¸ì¶œ
                print(f"[UPDATE] {month} ìºì‹œ ì—†ìŒ â†’ API í˜¸ì¶œ")
                start = datetime(year, m, 1).strftime("%Y-%m-%d")
                if m == 12:
                    end = datetime(year+1, 1, 1) - timedelta(days=1)
                else:
                    end = datetime(year, m+1, 1) - timedelta(days=1)
                end = end.strftime("%Y-%m-%d")
                try:
                    userchats = await channel_api.get_userchats(start, end)
                    if userchats:
                        df = await channel_api.process_userchat_data(userchats)
                        df = attach_resolution_fallback(df)
                        meta = {"month": month, "range": [start, end], "api_fetch": True}
                        server_cache.save_data(f"userchats_{month}", df, meta)
                        all_data.append(df)
                except Exception as e:
                    print(f"[UPDATE] {month} API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        else:  # refresh_mode == "cache"
            # ìºì‹œë§Œ ì‚¬ìš©: API í˜¸ì¶œ ì•ˆ í•¨
            df = get_cached_data_month(month)
            if df is not None and not df.empty:
                print(f"[CACHE] {month} ë¡œë“œ ({len(df)} rows)")
                all_data.append(df)
            else:
                print(f"[CACHE] {month} ì—†ìŒ (API í˜¸ì¶œ ì•ˆ í•¨)")
                
    print(f"[DEBUG] all_data ê°œìˆ˜: {len(all_data)}")
    print(f"[DEBUG] all_data ê° ì›”ë³„ ê°œìˆ˜: {[len(df) if df is not None else 0 for df in all_data]}")
    
    if not all_data:
        print(f"[DEBUG] all_dataê°€ ë¹„ì–´ìˆìŒ - ë¹ˆ DataFrame ë°˜í™˜")
        return pd.DataFrame()
    
    print(f"[DEBUG] pd.concat ì‹œì‘ - ì´ {sum(len(df) if df is not None else 0 for df in all_data)} rows)")
    try:
        combined = pd.concat(all_data, ignore_index=True)
        print(f"[DEBUG] pd.concat ì™„ë£Œ: {len(combined)} rows")
    except Exception as e:
        print(f"[ERROR] pd.concat ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()
    
    print(f"[DEBUG] firstAskedAt ì»¬ëŸ¼ ë³€í™˜ ì‹œì‘")
    try:
        combined['firstAskedAt'] = pd.to_datetime(combined['firstAskedAt'], errors='coerce')
        print(f"[DEBUG] firstAskedAt ë³€í™˜ ì™„ë£Œ: {combined['firstAskedAt'].notna().sum()}ê°œ ìœ íš¨, {combined['firstAskedAt'].isna().sum()}ê°œ NaN")
    except Exception as e:
        print(f"[ERROR] firstAskedAt ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()
    
    # firstAskedAt ì²˜ë¦¬ ì•„ë˜ì— ì´ì–´ì„œ
    for col in ["createdAt","openedAt","closedAt"]:
        if col in combined.columns:
            try:
                combined[col] = pd.to_datetime(combined[col], errors='coerce')
            except Exception:
                pass
    
    print(f"[DEBUG] ì¤‘ë³µ ì œê±° ì‹œì‘")
    try:
        before_dedup = len(combined)
        if 'userChatId' in combined.columns:
            combined = combined.drop_duplicates(subset=['userChatId'], keep='first')
        else:
            combined = combined.drop_duplicates(subset=['userId', 'firstAskedAt', 'createdAt'], keep='first')
        after_dedup = len(combined)
        print(f"[DEBUG] ì¤‘ë³µ ì œê±° ì™„ë£Œ: {before_dedup} â†’ {after_dedup} rows")
    except Exception as e:
        print(f"[ERROR] ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()

    print(f"[DEBUG] ë‚ ì§œ í•„í„°ë§ ì‹œì‘")
    try:
        # 1) firstAskedAt â†’ KST naive ë¡œ ì •ê·œí™”
        fa = pd.to_datetime(combined['firstAskedAt'], errors='coerce')
        # tz-aware(ì˜ˆ: UTC) ì¸ ê²½ìš°ë§Œ Asia/Seoulë¡œ ë³€í™˜ í›„ tz ì œê±°
        if getattr(fa.dt, "tz", None) is not None:
            fa = fa.dt.tz_convert('Asia/Seoul').dt.tz_localize(None)
        combined['firstAskedAt'] = fa

        # 2) ë¹„êµ ë²”ìœ„ (KST naive)
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)

        # 3) ë””ë²„ê·¸: íŠ¹ì • ë‚ ì§œ ì ê²€(ì›í•˜ì‹œë©´ ë‚ ì§œ ë°”ê¿”ì„œ ë³´ì„¸ìš”)
        # íŠ¹ì • ì¼ì ì¹´ìš´íŠ¸(ì˜ˆ: 2025-08-19)
        if start_date == end_date:
            target_date = pd.to_datetime(start_date).date()
            same_day = (combined['firstAskedAt'].dt.date == target_date).sum()
            print(f"[DEBUG] same-day({target_date}) rows(before mask): {same_day}")

        print(f"[DEBUG] ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date} (KST naive) â†’ {start_dt} ~ {end_dt}")
        mask = (combined['firstAskedAt'] >= start_dt) & (combined['firstAskedAt'] <= end_dt)
        print(f"[DEBUG] ë§ˆìŠ¤í¬ ì ìš©: {mask.sum()}ê°œ True, {len(mask) - mask.sum()}ê°œ False")
        
        result = combined[mask].reset_index(drop=True)
        print(f"[FILTER] ë‚ ì§œ í•„í„°ë§ ì™„ë£Œ: {start_date} ~ {end_date} (KST) â†’ {start_dt} ~ {end_dt} (KST)")
        print(f"[FILTER] í•„í„°ë§ ì „: {len(combined)} rows, í•„í„°ë§ í›„: {len(result)} rows")
        
    except Exception as e:
        print(f"[ERROR] ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return pd.DataFrame()
    
    return result

# === ì‹ ê·œ: CSAT ìºì‹œ ë¹Œë“œ ===
async def build_and_cache_csat_rows(start_date: str, end_date: str) -> int:
    """
    userchats ìºì‹œ(í˜¹ì€ ë°©ê¸ˆ ìƒˆë¡œê³ ì¹¨ëœ ë°ì´í„°)ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
    ê° userChatì˜ messagesë¥¼ ì¡°íšŒ â†’ CSAT ì„¤ë¬¸ íŒŒì‹± â†’ ì›”ë³„ csat ìºì‹œ ì €ì¥.
    ë°˜í™˜: ì €ì¥ëœ row ì´ ê°œìˆ˜
    """
    # ì¤‘ë³µ ì‹¤í–‰ ê°€ë“œ
    if csat_build_lock.locked():
        print("[CSAT] build already running â€” skip this trigger")
        return 0
    
    async with csat_build_lock:
        # 1) ì‚¬ìš©ì ë¬¸ì˜ ë°ì´í„° í™•ë³´(ìºì‹œ ê¸°ì¤€, API í˜¸ì¶œ ì—†ì´ ìš°ì„  ì‹œë„)
        user_df = await get_cached_data(start_date, end_date, refresh_mode="cache")
        if user_df is None or user_df.empty:
            # ê·¸ë˜ë„ ì—†ë‹¤ë©´, ì´ í•¨ìˆ˜ëŠ” ìƒˆë¡œê³ ì¹¨ ê²½ë¡œì—ì„œë§Œ í˜¸ì¶œí•˜ë„ë¡ ì„¤ê³„í–ˆìœ¼ë‹ˆ
            # ê°•ì œ ìƒˆë¡œê³ ì¹¨(=API í˜¸ì¶œ)ë¡œ í•œë²ˆ ì±„ì›Œì¤€ë‹¤.
            user_df = await get_cached_data(start_date, end_date, refresh_mode="refresh")
            if user_df is None or user_df.empty:
                return 0

        # 2) CSAT ë°ì´í„°ê°€ ìˆëŠ” userChatë§Œ í•„í„°ë§ (triggerId: 768201)
        print(f"[CSAT] ì „ì²´ userChat ìˆ˜: {len(user_df)}")
        
        # userChat ë ˆë²¨ì—ì„œëŠ” workflowIdê°€ ì•„ë‹ˆë¼ triggerIdë¥¼ í™•ì¸í•´ì•¼ í•¨
        # í•˜ì§€ë§Œ userChat ìì²´ì—ëŠ” triggerIdê°€ ì—†ê³ , ë©”ì‹œì§€ ë ˆë²¨ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥
        # ë”°ë¼ì„œ ëª¨ë“  userChatì„ ëŒ€ìƒìœ¼ë¡œ í•˜ê³ , ë©”ì‹œì§€ ë ˆë²¨ì—ì„œ í•„í„°ë§
        csat_df = user_df.copy()
        print(f"[CSAT] ì „ì²´ userChat ëŒ€ìƒ (ë©”ì‹œì§€ ë ˆë²¨ì—ì„œ triggerId í•„í„°ë§)")

        # 3) ìµœê·¼ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì •ë ¬ (íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´)
        csat_df = csat_df.sort_values("firstAskedAt", ascending=False)
        
        # 4) chatId ëª©ë¡ê³¼ userId/firstAskedAt ë§¤í•‘
        need_cols = ["userChatId", "userId", "firstAskedAt"]
        for c in need_cols:
            if c not in csat_df.columns:
                csat_df[c] = None
        sub = csat_df[need_cols].dropna(subset=["userChatId"]).drop_duplicates()

        # 5) ê° chatIdì˜ ë©”ì‹œì§€ ì¡°íšŒ & CSAT íŒŒì‹±
        rows = []
        total_n = len(sub)
        for i, (_, row) in enumerate(sub.iterrows(), start=1):
            chat_id = str(row["userChatId"])
            user_id = str(row["userId"]) if pd.notna(row["userId"]) else None
            asked_at = pd.to_datetime(row["firstAskedAt"], errors="coerce")
            if not chat_id or pd.isna(asked_at):
                continue

            try:
                print(f"[CSAT] ì²˜ë¦¬ ì¤‘: {chat_id} ({i}/{total_n})")
                msgs = await channel_api.get_messages_by_chat(chat_id, limit=500, sort_order="desc")
                # âœ… CSAT ì„¤ë¬¸ ì›Œí¬í”Œë¡œìš°(768201)ë§Œ íŒŒì‹± (ì¡ìŒ ì œê±°)
                cs = channel_api.extract_csat_from_messages(msgs, allowed_trigger_ids={"768201"})
                # âœ… 768201ì„ ì‹œì‘í•œ ê±´ì€ ì ìˆ˜ ì œì¶œ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ ì €ì¥ ëŒ€ìƒ
                if not cs or not cs.get("wf_768201_started"):
                    continue

                rows.append({
                    "firstAskedAt": asked_at,
                    "userId": user_id,
                    "userChatId": chat_id,
                    "personId": cs.get("personId"),
                    "A-1": cs.get("A-1"),
                    "A-2": cs.get("A-2"),
                    "comment_3": cs.get("comment_3"),
                    "A-4": cs.get("A-4"),
                    "A-5": cs.get("A-5"),
                    "comment_6": cs.get("comment_6"),
                    "csatSubmittedAt": cs.get("csatSubmittedAt"),
                    "csatDate": pd.to_datetime(cs.get("csatSubmittedAt"), errors="coerce"),
                    # ğŸ‘‰ ê³µí†µ ë¶„ëª¨ ê³„ì‚°ìš© í”Œë˜ê·¸ ì €ì¥
                    "wf_768201_started": bool(cs.get("wf_768201_started")),
                    "has_score_any": bool(cs.get("has_score_any")),
                })
            except Exception as e:
                print(f"[CSAT] chatId={chat_id} íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue

        if not rows:
            print("[CSAT] íŒŒì‹±ëœ CSAT ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        print(f"[CSAT] ì´ {len(rows)}ê°œì˜ CSAT ì‘ë‹µ íŒŒì‹± ì™„ë£Œ")

        # 6) ì›”ë³„ë¡œ ìª¼ê°œì„œ ì €ì¥ (userchats ìºì‹œì™€ ë™ì¼ ì •ì±…)
        csat_df = pd.DataFrame(rows)

        # === ê¸°ì¡´ buggy ë¸”ë¡ ì§€ìš°ê³  ì•„ë˜ë¡œ êµì²´ ===
        # 1) ë‘ ë‚ ì§œ ì»¬ëŸ¼ì„ ê°ê° KST naiveë¡œ ì •ê·œí™”
        first_dt = pd.to_datetime(csat_df["firstAskedAt"], errors="coerce")
        if getattr(first_dt.dt, "tz", None) is not None:
            first_dt = first_dt.dt.tz_convert("Asia/Seoul").dt.tz_localize(None)

        # csatSubmittedAtì—ì„œ ë§Œë“  csatDateëŠ” KST(+09:00) tz-aware ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ
        # â†’ utc=Trueë¡œ íŒŒì‹± í›„ KSTë¡œ ë³€í™˜, tz ì œê±°
        csat_dt = pd.to_datetime(csat_df["csatDate"], errors="coerce", utc=True)
        csat_dt = csat_dt.dt.tz_convert("Asia/Seoul").dt.tz_localize(None)

        # 2) ì œì¶œì¼ ìš°ì„ , ì—†ìœ¼ë©´ firstAskedAt
        csat_df["bucketDate"] = csat_dt.fillna(first_dt)

        # 3) ë²„í‚· ì—†ëŠ” í–‰ ì œì™¸ í›„ month ìƒì„±
        csat_df = csat_df[csat_df["bucketDate"].notna()].copy()
        csat_df["month"] = csat_df["bucketDate"].dt.to_period("M").astype(str)
        total_saved = 0
        for month, mdf in csat_df.groupby("month"):
            mdf = mdf.drop(columns=["month", "bucketDate"])
            key = f"csat_{month}"
            meta = {"month": month, "range": [start_date, end_date], "api_fetch": True, "kind": "csat"}
            server_cache.save_data(key, mdf, meta)
            total_saved += len(mdf)   # âœ… ëˆ„ì  ì €ì¥ ìˆ˜ ë°˜ì˜
        print(f"[CSAT] ì´ {total_saved} rows ì €ì¥ ì™„ë£Œ")
        return total_saved

# === ì‹ ê·œ: csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œ (triggerId í•„í„°ë§) ===
def load_csat_raw_data() -> Optional[pd.DataFrame]:
    """csat_raw.pklì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. triggerId: 768201ë§Œ ìœ íš¨."""
    try:
        df, meta = server_cache.load_data("csat_raw")
        if df is None or df.empty:
            print(f"[CSAT] csat_raw.pkl ë°ì´í„° ì—†ìŒ")
            return None
        
        # triggerId ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if "triggerId" in df.columns:
            # triggerIdê°€ 768201ì¸ ë°ì´í„°ë§Œ í•„í„°ë§
            filtered_df = df[df["triggerId"] == "768201"].copy()
            print(f"[CSAT] csat_raw.pkl ë¡œë“œ ì„±ê³µ: {len(df)} rows â†’ triggerId 768201 í•„í„°ë§: {len(filtered_df)} rows")
            return filtered_df
        else:
            # triggerId ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            print(f"[CSAT] csat_raw.pkl ë¡œë“œ ì„±ê³µ: {len(df)} rows (triggerId ì»¬ëŸ¼ ì—†ìŒ)")
            return df
            
    except Exception as e:
        print(f"[CSAT] csat_raw.pkl ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# === ì‹ ê·œ: ìºì‹œ ë¡œë”(ìºì‹œ ì „ìš©) - ìˆ˜ì •ë¨ ===
def load_csat_rows_from_cache(start_date: str, end_date: str) -> pd.DataFrame:
    """
    CSAT ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    1. ë¨¼ì € csat_YYYY-MM.pkl íŒŒì¼ë“¤ì„ ì°¾ì•„ë´…ë‹ˆë‹¤
    2. ì—†ìœ¼ë©´ csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤
    """
    def _months(s, e):
        sm = pd.to_datetime(s).to_period('M')
        em = pd.to_datetime(e).to_period('M')
        cur = sm
        out = []
        while cur <= em:
            out.append(str(cur))
            cur += 1
        return out

    # 1ë‹¨ê³„: ì›”ë³„ ìºì‹œ íŒŒì¼ë“¤ì—ì„œ ë¡œë“œ ì‹œë„
    months = _months(start_date, end_date)
    frames = []
    for month in months:
        key = f"csat_{month}"
        df, _ = server_cache.load_data(key)
        if df is not None and not df.empty:
            frames.append(df)
    
    if frames:
        # ì›”ë³„ ìºì‹œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        print(f"[CSAT] ì›”ë³„ ìºì‹œì—ì„œ {len(frames)}ê°œ íŒŒì¼ ë¡œë“œ")
        out = pd.concat(frames, ignore_index=True)
    else:
        # 2ë‹¨ê³„: csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œ
        print(f"[CSAT] ì›”ë³„ ìºì‹œ ì—†ìŒ, csat_raw.pklì—ì„œ ì§ì ‘ ë¡œë“œ")
        raw_df = load_csat_raw_data()
        if raw_df is None or raw_df.empty:
            return pd.DataFrame()
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
        date_col = None
        for col in raw_df.columns:
            if any(keyword in col.lower() for keyword in ['date', 'ë‚ ì§œ', 'created', 'ìƒì„±', 'firstasked']):
                date_col = col
                break
        
        if date_col is None:
            print(f"[CSAT] ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {list(raw_df.columns)}")
            return pd.DataFrame()
        
        # ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
        try:
            raw_df[date_col] = pd.to_datetime(raw_df[date_col], errors='coerce')
            s = pd.to_datetime(start_date)
            e = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)
            out = raw_df[(raw_df[date_col].notna()) & (raw_df[date_col] >= s) & (raw_df[date_col] <= e)].copy()
            print(f"[CSAT] csat_raw.pklì—ì„œ {len(out)} rows í•„í„°ë§ ì™„ë£Œ")
        except Exception as e:
            print(f"[CSAT] ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    # âœ… ê¸°ê°„ í•„í„°: ì œì¶œì¼(ìš°ì„ ) â†’ ì œì¶œì¼ì´ ì—†ìœ¼ë©´ firstAskedAt
    # âœ… tz-aware(ì˜ˆ: +09:00) â†’ Asia/Seoulë¡œ ë³€í™˜ í›„ naive ë¹„êµ
    date_candidates = [c for c in ["csatDate", "csatSubmittedAt", "submittedAt", "firstAskedAt"] if c in out.columns]
    date_key = date_candidates[0] if date_candidates else None

    def _to_kst_naive_series(s):
        dt = pd.to_datetime(s, errors="coerce")
        # tz-awareë©´ KSTë¡œ ë³€í™˜ í›„ tz ì œê±°
        try:
            if getattr(dt.dt, "tz", None) is not None:
                dt = dt.dt.tz_convert("Asia/Seoul").dt.tz_localize(None)
        except Exception:
            # ì¼ë¶€ ì¼€ì´ìŠ¤ëŠ” tz_localizeë¡œë§Œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ìˆì–´ ë³´ì¡° ì²˜ë¦¬
            try:
                dt = pd.to_datetime(s, errors="coerce", utc=True).dt.tz_convert("Asia/Seoul").dt.tz_localize(None)
            except Exception:
                pass
        return dt

    if date_key is not None:
        out["_csat_dt"] = _to_kst_naive_series(out[date_key])

        # ëª¨ë“  ê°’ì´ NaTë©´ firstAskedAtë¡œ í´ë°± ì‹œë„
        if out["_csat_dt"].notna().sum() == 0 and "firstAskedAt" in out.columns:
            out["_csat_dt"] = _to_kst_naive_series(out["firstAskedAt"])

        s = pd.to_datetime(start_date)
        e = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(milliseconds=1)

        mask = out["_csat_dt"].notna() & (out["_csat_dt"] >= s) & (out["_csat_dt"] <= e)
        out = out.loc[mask].drop(columns=["_csat_dt"]).reset_index(drop=True)
    else:
        # ë‚ ì§œ í‚¤ê°€ ì „í˜€ ì—†ìœ¼ë©´ í•„í„°ë¥¼ ê±´ë„ˆëœ€(í…… ë¹„ëŠ” ê²ƒ ë°©ì§€)
        print("[CSAT] ê²½ê³ : ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•´ ê¸°ê°„ í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    return out

def get_filtered_df(df: pd.DataFrame, ê³ ê°ìœ í˜•="ì „ì²´", ê³ ê°ìœ í˜•_2ì°¨="ì „ì²´", ë¬¸ì˜ìœ í˜•="ì „ì²´", 
                   ë¬¸ì˜ìœ í˜•_2ì°¨="ì „ì²´", ì„œë¹„ìŠ¤ìœ í˜•="ì „ì²´", ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨="ì „ì²´") -> pd.DataFrame:
    temp = df.copy()
    required_columns = ["ê³ ê°ìœ í˜•","ê³ ê°ìœ í˜•_2ì°¨","ë¬¸ì˜ìœ í˜•","ë¬¸ì˜ìœ í˜•_2ì°¨","ì„œë¹„ìŠ¤ìœ í˜•","ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨"]
    for col in required_columns:
        if col not in temp.columns:
            temp[col] = None
    
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‹¤ì¤‘ ì„ íƒ ì²˜ë¦¬ (OR ì¡°ê±´)
    if ê³ ê°ìœ í˜• != "ì „ì²´": 
        ê³ ê°ìœ í˜•_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ê³ ê°ìœ í˜•.split(',') if v.strip()]
        if ê³ ê°ìœ í˜•_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ê³ ê°ìœ í˜•"].isin(ê³ ê°ìœ í˜•_ë¦¬ìŠ¤íŠ¸)]
    
    if ê³ ê°ìœ í˜•_2ì°¨ != "ì „ì²´": 
        ê³ ê°ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ê³ ê°ìœ í˜•_2ì°¨.split(',') if v.strip()]
        if ê³ ê°ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ê³ ê°ìœ í˜•_2ì°¨"].isin(ê³ ê°ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸)]
    
    if ë¬¸ì˜ìœ í˜• != "ì „ì²´": 
        ë¬¸ì˜ìœ í˜•_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ë¬¸ì˜ìœ í˜•.split(',') if v.strip()]
        if ë¬¸ì˜ìœ í˜•_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ë¬¸ì˜ìœ í˜•"].isin(ë¬¸ì˜ìœ í˜•_ë¦¬ìŠ¤íŠ¸)]
    
    if ë¬¸ì˜ìœ í˜•_2ì°¨ != "ì „ì²´": 
        ë¬¸ì˜ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ë¬¸ì˜ìœ í˜•_2ì°¨.split(',') if v.strip()]
        if ë¬¸ì˜ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ë¬¸ì˜ìœ í˜•_2ì°¨"].isin(ë¬¸ì˜ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸)]
    
    if ì„œë¹„ìŠ¤ìœ í˜• != "ì „ì²´": 
        ì„œë¹„ìŠ¤ìœ í˜•_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ì„œë¹„ìŠ¤ìœ í˜•.split(',') if v.strip()]
        if ì„œë¹„ìŠ¤ìœ í˜•_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ì„œë¹„ìŠ¤ìœ í˜•"].isin(ì„œë¹„ìŠ¤ìœ í˜•_ë¦¬ìŠ¤íŠ¸)]
    
    if ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ != "ì „ì²´": 
        ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸ = [v.strip() for v in ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨.split(',') if v.strip()]
        if ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸:
            temp = temp[temp["ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨"].isin(ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨_ë¦¬ìŠ¤íŠ¸)]
    required_keys = ["userId","mediumType","workflowId","tags","firstAskedAt"]
    for k in required_keys:
        if k not in temp.columns:
            temp[k] = None
    for k in ["operationWaitingTime","operationAvgReplyTime","operationTotalReplyTime","operationResolutionTime"]:
        if k in temp.columns:
            temp[k] = temp[k].apply(lambda x: x if isinstance(x, str) else None)
    return temp.reset_index(drop=True)

def enrich_csat_with_user_types(csat_df: pd.DataFrame, chats_df: pd.DataFrame) -> pd.DataFrame:
    """
    csat_df:  personId, A-1, A-2, ...
    chats_df: userId, ë¬¸ì˜ìœ í˜•, ë¬¸ì˜ìœ í˜•_2ì°¨, ê³ ê°ìœ í˜•, ...
    ë°˜í™˜:     userId, ìœ í˜• ì»¬ëŸ¼ + CSAT ë¬¸í•­ì´ í•©ì³ì§„ DF
    """
    try:
        if csat_df is None or chats_df is None:
            print("[CSAT] ì…ë ¥ ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤.")
            return pd.DataFrame()
        
        if csat_df.empty or chats_df.empty:
            print("[CSAT] ì…ë ¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        # userId í•„ìˆ˜ ë°©ì–´ (CSAT ë°ì´í„°)
        if "userId" not in csat_df.columns:
            raise ValueError("csat_dfì— userIdê°€ ì—†ìŠµë‹ˆë‹¤.")
        if "userId" not in chats_df.columns:
            raise ValueError("chats_dfì— userIdê°€ ì—†ìŠµë‹ˆë‹¤.")

        print(f"[CSAT] CSAT ë°ì´í„° {len(csat_df)}ê±´ì— ìœ í˜• ì •ë³´ ì¶”ê°€ ì‹œì‘...")
        
        # CSAT ë°ì´í„°ëŠ” ì´ë¯¸ userIdê°€ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        csat_df_copy = csat_df.copy()
        
        # ì¡°ì¸ì— í•„ìš”í•œ ìµœì†Œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (1ì°¨ ë¶„ë¥˜ë§Œ ì‚¬ìš©)
        need_cols = ["userId", "ë¬¸ì˜ìœ í˜•", "ê³ ê°ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•"]
        use_cols = [c for c in need_cols if c in chats_df.columns]
        
        # userIdë¡œ ì´ë„ˆì¡°ì¸
        merged = pd.merge(
            csat_df_copy,
            chats_df[use_cols].drop_duplicates(subset=["userId"], keep="last"),
            on="userId",
            how="inner",
        )

        # ì¤‘ë³µ CSAT ì‘ë‹µ ì •ë¦¬: userChatId+csatSubmittedAt ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        if "csatSubmittedAt" in merged.columns and "userChatId" in merged.columns:
            merged = (merged
                      .sort_values(["userChatId", "csatSubmittedAt"])
                      .drop_duplicates(subset=["userChatId", "csatSubmittedAt"], keep="last"))
        
        # ë§¤ì¹­ëœ ê±´ìˆ˜ í™•ì¸
        matched_count = len(merged)
        print(f"[CSAT] ìœ í˜• ì •ë³´ ë§¤ì¹­ ì™„ë£Œ: {matched_count}/{len(csat_df)}ê±´ ({matched_count/len(csat_df)*100:.1f}%)")
        
        return merged
        
    except Exception as e:
        print(f"[CSAT] ìœ í˜• ì •ë³´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def safe_mean(series):
    """ì•ˆì „í•œ í‰ê·  ê³„ì‚° í•¨ìˆ˜ - NaN/inf ê°’ ë°©ì§€"""
    try:
        if series is None or series.empty:
            return 0.0
        non_null = series.dropna()
        if len(non_null) == 0:
            return 0.0
        mean_val = non_null.mean()
        if pd.notna(mean_val) and np.isfinite(mean_val):
            return float(mean_val)
        else:
            return 0.0
    except:
        return 0.0

def build_csat_type_scores(enriched_df: pd.DataFrame):
    """
    enriched_df: enrich_csat_with_user_types() ê²°ê³¼
      í¬í•¨ ì»¬ëŸ¼: personId, userId, ë¬¸ì˜ìœ í˜•, ë¬¸ì˜ìœ í˜•_2ì°¨, ê³ ê°ìœ í˜•, A-1, A-2, ...
    ë°˜í™˜: {
      "ë¬¸ì˜ìœ í˜•": { "A-1": [ { "ë¬¸ì˜ìœ í˜•": "...", "í‰ê· ì ìˆ˜": 4.7, "ì‘ë‹µììˆ˜": 12, "userIds": [...] }, ... ], ... },
      "ê³ ê°ìœ í˜•": { ... }
    }
    """
    import pandas as pd
    import numpy as np
    
    if enriched_df is None or enriched_df.empty:
        print(f"[CSAT] enriched_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {}

    print(f"[CSAT] ìœ í˜•ë³„ ì§‘ê³„ ì‹œì‘: {len(enriched_df)}ê±´")
    
    csat_cols = [c for c in enriched_df.columns if c.startswith("A-")]
    print(f"  - CSAT ë¬¸í•­ ì»¬ëŸ¼: {csat_cols}")
    
    result = {}

    def _group_payload(df, label_col, score_col):
        # --- 1) wf_768201_startedë¥¼ ê²¬ê³ í•œ bool ì‹œë¦¬ì¦ˆë¡œ ì •ê·œí™” ---
        raw = df["wf_768201_started"] if "wf_768201_started" in df.columns else pd.Series(False, index=df.index)
        elig = pd.Series(raw, index=df.index)  # ì¸ë±ìŠ¤ ì •ë ¬
        elig = (elig
                .replace({True: True, False: False,
                          'True': True, 'False': False,
                          'true': True, 'false': False,
                          '1': True, '0': False, 1: True, 0: False})
                .fillna(False)
                .astype(bool))

        # ì´ ë¬¸í•­ ê°’ë§Œ ìˆ«ìí™” (ì‘ë‹µ ìœ ë¬´ íŒë‹¨ìš©)
        tmp = pd.to_numeric(df[score_col], errors="coerce")

        records = []
        # dropna=False ë¡œ ê·¸ë£¹í•‘í•´ì•¼ NaN ë¼ë²¨(ë¹ˆ ê°’)ë„ ë”°ë¡œ ì§‘ê³„ ê°€ëŠ¥
        for label_val, sub in df.groupby(label_col, dropna=False):
            sub_idx = sub.index

            # --- 2) ê³µí†µ ë¶„ëª¨: ì„¤ë¬¸ ì‹œì‘ì ìˆ˜ (í•´ë‹¹ ê·¸ë£¹ ë²”ìœ„ë¡œ ì¸ë±ìŠ¤ ë§ì¶° í•©ì‚°) ---
            denom = int(elig.reindex(sub_idx).sum())

            # --- 3) ë¬¸í•­ë³„ ì‘ë‹µììˆ˜: ì´ ë¬¸í•­ì— ì‹¤ì œ ê°’ì´ ìˆëŠ” ì‚¬ëŒ ìˆ˜ ---
            answered_this = int(tmp.reindex(sub_idx).notna().sum())

            # --- 4) ë¬¸í•­ë³„ ë¯¸ì‘ë‹µììˆ˜ ---
            non_responded = max(0, denom - answered_this)

            # ë””ë²„ê·¸ ë¡œê·¸
            print(f"[CSAT_GRP] {label_col}={label_val} denom={denom} answered_this({score_col})={answered_this}")

            # í‰ê· ì ìˆ˜(ë¬¸í•­ë³„) ê³„ì‚°
            series = pd.to_numeric(sub[score_col], errors='coerce').dropna()
            avg = float(series.mean()) if len(series) else 0.0
            if not np.isfinite(avg):
                avg = 0.0

            user_ids = sub.get("userId")
            user_ids = sorted(set(user_ids.dropna().astype(str).tolist()))[:50] if user_ids is not None else []

            records.append({
                label_col: label_val,
                "í‰ê· ì ìˆ˜": avg,
                "userIds": user_ids,

                # âœ… ê³µí†µ ë¶„ëª¨ + ë¬¸í•­ë³„ ì‘ë‹µ/ë¯¸ì‘ë‹µ
                "ëŒ€ìƒììˆ˜": denom,
                "ì‘ë‹µììˆ˜": answered_this,
                "ë¯¸ì‘ë‹µììˆ˜": non_responded,
                "ë§‰ëŒ€ê°’": answered_this,  # ì°¨íŠ¸ ë§‰ëŒ€ ê¸¸ì´ = ë¬¸í•­ë³„ ì‘ë‹µììˆ˜
            })

        # ì•ˆì „ì¥ì¹˜
        for r in records:
            r["í‰ê· ì ìˆ˜"] = float(np.nan_to_num(r["í‰ê· ì ìˆ˜"], nan=0.0, posinf=0.0, neginf=0.0))
            r["ì‘ë‹µììˆ˜"] = int(np.nan_to_num(r["ì‘ë‹µììˆ˜"], nan=0, posinf=0, neginf=0))
            r["ë¯¸ì‘ë‹µììˆ˜"] = int(np.nan_to_num(r["ë¯¸ì‘ë‹µììˆ˜"], nan=0, posinf=0, neginf=0))
            r["ëŒ€ìƒììˆ˜"] = int(np.nan_to_num(r["ëŒ€ìƒììˆ˜"], nan=0, posinf=0, neginf=0))
            r["ë§‰ëŒ€ê°’"]   = int(np.nan_to_num(r["ë§‰ëŒ€ê°’"],   nan=0, posinf=0, neginf=0))

        return sorted(records, key=lambda r: r["ì‘ë‹µììˆ˜"], reverse=True)

    for label in ["ë¬¸ì˜ìœ í˜•", "ê³ ê°ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•"]:
        result[label] = {}
        for a in csat_cols:
            result[label][a] = _group_payload(enriched_df, label, a)
            print(f"  - {label}ë³„ {a}: {len(result[label][a])}ê°œ ê·¸ë£¹")

    print(f"[CSAT] ìœ í˜•ë³„ ì§‘ê³„ ì™„ë£Œ: {len(result)}ê°œ ìœ í˜•")
    return result