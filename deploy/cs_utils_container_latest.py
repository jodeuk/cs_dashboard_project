import os
import httpx
import pandas as pd
import json
import pickle
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import asyncio
from dotenv import load_dotenv

print("====[CS_UTILS.PY ì½”ë“œê°€ Dockerì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤]====")

CACHE_EXPIRE_HOURS = 24

def get_cache_directory():
    is_docker = os.getenv('DOCKER_ENV') or os.path.exists('/.dockerenv')
    is_render = os.getenv('RENDER') or os.path.exists('/opt/render')
    
    if is_docker or is_render:
        cache_dir = os.getenv('CACHE_DIR', '/data/cache')
        print(f"[DEBUG] Docker/Render í™˜ê²½ ê°ì§€ë¨ - Persistent Disk ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    else:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        cache_dir = os.path.join(project_root, 'cache')
        print(f"[DEBUG] ë¡œì»¬ í™˜ê²½ - ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir, exist_ok=True)
        print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {cache_dir}")
    
    return cache_dir

CACHE_DIR = get_cache_directory()
print(f"[DEBUG] ìµœì¢… ì„¤ì •ëœ ìºì‹œ ë””ë ‰í† ë¦¬: {CACHE_DIR}")
print(f"[DEBUG] ìºì‹œ ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼: {os.listdir(CACHE_DIR) if os.path.exists(CACHE_DIR) else 'ë””ë ‰í† ë¦¬ ì—†ìŒ'}")

load_dotenv()

class ChannelTalkAPI:
    def __init__(self):
        self.base_url = "https://api.channel.io"
        self.access_key = os.getenv("CHANNEL_ACCESS_KEY")
        self.access_secret = os.getenv("CHANNEL_ACCESS_SECRET")
        self.headers = {
            "x-access-key": self.access_key,
            "x-access-secret": self.access_secret,
            "Content-Type": "application/json"
        }

    async def get_all_users(self, limit: int = 100) -> List[Dict]:
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        url = f"{self.base_url}/open/v5/users"
        params = {"limit": limit}
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            data = response.json()
            return data.get('users', [])

    async def get_userchats(self, start_date: str, end_date: str, limit: int = 1000) -> List[Dict]:
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_timestamp = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp() * 1000)
        end_timestamp = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp() * 1000)
        
        all_userchats = []
        since = None
        page_count = 0
        max_pages = 10
        collected_ids = set()
        last_next = None
        consecutive_same_next = 0
        
        try:
            while page_count < max_pages:
                page_count += 1
                url = f"{self.base_url}/open/v5/user-chats"
                params = {"limit": limit, "state": "closed"}
                if since:
                    params["since"] = since
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(url, headers=self.headers, params=params)
                    response.raise_for_status()
                    data = response.json()
                    user_chats = data.get('userChats', [])
                    next_value = data.get('next', None)
                    
                    print(f"[API] {page_count}ë²ˆì§¸ | since: {since} | userChats: {len(user_chats)} | next: {next_value}")
                    
                    if not user_chats:
                        print("[API] ë” ì´ìƒ userChats ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    filtered_chats = []
                    for chat in user_chats:
                        chat_id = chat.get("id")
                        if chat_id not in collected_ids:
                            first_asked_at = chat.get("firstAskedAt")
                            if first_asked_at and start_timestamp <= first_asked_at <= end_timestamp:
                                filtered_chats.append(chat)
                                collected_ids.add(chat_id)
                    
                    all_userchats.extend(filtered_chats)
                    print(f"[API] ë‚ ì§œ í•„í„°ë§ í›„ ì¶”ê°€ëœ ì±„íŒ…: {len(filtered_chats)}")
                    
                    if not next_value or not str(next_value).strip():
                        print("[API] next ì—†ìŒ, ì¢…ë£Œ")
                        break
                    
                    if next_value == last_next:
                        consecutive_same_next += 1
                        print(f"[API] ë™ì¼ next ë°˜ë³µ {consecutive_same_next}íšŒ | next: {next_value}")
                        if consecutive_same_next >= 2:
                            print("[API] ë¬´í•œë£¨í”„ ë°©ì§€, ì¢…ë£Œ")
                            break
                    else:
                        consecutive_same_next = 0
                    
                    if user_chats:
                        latest_chat = user_chats[0]
                        latest_timestamp = latest_chat.get("firstAskedAt")
                        if latest_timestamp and latest_timestamp < start_timestamp:
                            print(f"[API] ìµœì‹  ë°ì´í„°({latest_timestamp})ê°€ ìš”ì²­ ê¸°ê°„({start_timestamp})ë³´ë‹¤ ì´ì „, ì¢…ë£Œ")
                            break
                    
                    since = next_value
                    last_next = next_value
                    
        except httpx.HTTPStatusError as e:
            print(f"HTTP Error: {e.response.status_code} - {e.response.text}")
            raise
        except Exception as e:
            print(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
        
        print(f"[API] ì´ ìˆ˜ì§‘ëœ ì±„íŒ… ìˆ˜: {len(all_userchats)} (ê¸°ê°„: {start_date} ~ {end_date})")
        return all_userchats

    async def get_userchat_by_id(self, userchat_id: str) -> Dict:
        if not self.access_key or not self.access_secret:
            raise ValueError("CHANNEL_ACCESS_KEY ë˜ëŠ” CHANNEL_ACCESS_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        url = f"{self.base_url}/open/v5/user-chats/{userchat_id}"
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, headers=self.headers)
            response.raise_for_status()
            return response.json()

    def hms_to_seconds(self, time_str: str) -> int:
        """HH:MM:SS í˜•ì‹ì˜ ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        try:
            hours, minutes, seconds = map(int, time_str.split(':'))
            return hours * 3600 + minutes * 60 + seconds
        except:
            return 0

    def extract_level(self, tags: List[str], type_name: str, level: int) -> Optional[str]:
        """íƒœê·¸ì—ì„œ íŠ¹ì • íƒ€ì…ì˜ ë ˆë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not tags:
            print(f"[EXTRACT] íƒœê·¸ ì—†ìŒ: {type_name}")
            return None
        
        print(f"[EXTRACT] íƒœê·¸ ëª©ë¡: {tags}")
        print(f"[EXTRACT] ì°¾ëŠ” íƒ€ì…: {type_name}, ë ˆë²¨: {level}")
        
        for tag in tags:
            if tag.startswith(f"{type_name}/"):
                parts = tag.split("/")
                if len(parts) > level:
                    result = parts[level]
                    print(f"[EXTRACT] ë§¤ì¹­ íƒœê·¸: {tag}, íŒŒíŠ¸: {parts}, ì¶”ì¶œ ê²°ê³¼: {result}")
                    return result
                else:
                    print(f"[EXTRACT] ë ˆë²¨ ë¶€ì¡±: {len(parts)} <= {level}")
        
        print(f"[EXTRACT] ë§¤ì¹­ íƒœê·¸ ì—†ìŒ: {type_name}")
        return None

    async def process_userchat_data(self, data: List[Dict]) -> pd.DataFrame:
        keep_keys = [
            "userId", "mediumType", "workflowId", "tags", "chats", "createdAt", 
            "firstAskedAt", "operationWaitingTime", "operationAvgReplyTime", 
            "operationTotalReplyTime", "operationResolutionTime"
        ]

        def convert_time(key, ms):
            if ms is None:
                return None
            try:
                if key == "firstAskedAt":
                    # ğŸ’¡ í•­ìƒ pandas.Timestampë¡œ ë³€í™˜!
                    if isinstance(ms, str):
                        return pd.to_datetime(ms, errors='coerce')
                    elif isinstance(ms, (int, float)):
                        return pd.to_datetime(ms, unit='ms')
                    elif isinstance(ms, (pd.Timestamp, datetime)):
                        return ms
                    else:
                        return pd.NaT
                else:
                    from datetime import timedelta
                    td = timedelta(milliseconds=ms)
                    total_seconds = int(td.total_seconds())
                    hours = total_seconds // 3600
                    minutes = (total_seconds % 3600) // 60
                    seconds = total_seconds % 60
                    return f"{hours:02}:{minutes:02}:{seconds:02}"
            except Exception as e:
                print(f"[convert_time] ì˜¤ë¥˜ key={key} value={ms}: {e}")
                return None

        processed_data = []
        
        for idx, item in enumerate(data):
            # firstAskedAt ì¡´ì¬ ì—¬ë¶€ ì²´í¬
            first_asked_at = item.get("firstAskedAt")
            if first_asked_at is None:
                print(f"[PROCESS] ì•„ì´í…œ #{idx} - firstAskedAt ì—†ìŒ, ìŠ¤í‚µ")
                continue
            
            print(f"[PROCESS] ì›ë³¸ ì•„ì´í…œ #{idx} í‚¤ë“¤: {list(item.keys())}")
            tags = item.get('tags', [])
            print(f"[PROCESS] ì•„ì´í…œ #{idx} tags: {tags}")

            new_obj = {}
            for key in keep_keys:
                value = item.get(key)
                if key == "workflowId":
                    value = item.get("source", {}).get("workflow", {}).get("id")
                elif key in ["firstAskedAt", "operationWaitingTime", "operationAvgReplyTime", "operationTotalReplyTime", "operationResolutionTime"]:
                    value = convert_time(key, value)
                new_obj[key] = value

            ê³ ê°ìœ í˜•_1ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 1)
            ê³ ê°ìœ í˜•_2ì°¨ = self.extract_level(tags, "ê³ ê°ìœ í˜•", 2)
            ë¬¸ì˜ìœ í˜•_1ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 1)
            ë¬¸ì˜ìœ í˜•_2ì°¨ = self.extract_level(tags, "ë¬¸ì˜ìœ í˜•", 2)
            ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 1)
            ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ = self.extract_level(tags, "ì„œë¹„ìŠ¤ìœ í˜•", 2)

            ê³ ê°ìœ í˜• = f"{ê³ ê°ìœ í˜•_1ì°¨}/{ê³ ê°ìœ í˜•_2ì°¨}" if ê³ ê°ìœ í˜•_1ì°¨ and ê³ ê°ìœ í˜•_2ì°¨ else ê³ ê°ìœ í˜•_1ì°¨
            ë¬¸ì˜ìœ í˜• = f"{ë¬¸ì˜ìœ í˜•_1ì°¨}/{ë¬¸ì˜ìœ í˜•_2ì°¨}" if ë¬¸ì˜ìœ í˜•_1ì°¨ and ë¬¸ì˜ìœ í˜•_2ì°¨ else ë¬¸ì˜ìœ í˜•_1ì°¨
            ì„œë¹„ìŠ¤ìœ í˜• = f"{ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨}/{ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨}" if ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨ and ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨ else ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨

            print(f"[PROCESS] ì¶”ì¶œ ê²°ê³¼ - ê³ ê°ìœ í˜•: {ê³ ê°ìœ í˜•}, ë¬¸ì˜ìœ í˜•: {ë¬¸ì˜ìœ í˜•}, ì„œë¹„ìŠ¤ìœ í˜•: {ì„œë¹„ìŠ¤ìœ í˜•}")

            processed_item = {
                **new_obj,
                "ê³ ê°ìœ í˜•": ê³ ê°ìœ í˜•,
                "ë¬¸ì˜ìœ í˜•": ë¬¸ì˜ìœ í˜•,
                "ì„œë¹„ìŠ¤ìœ í˜•": ì„œë¹„ìŠ¤ìœ í˜•,
                "ê³ ê°ìœ í˜•_1ì°¨": ê³ ê°ìœ í˜•_1ì°¨,
                "ë¬¸ì˜ìœ í˜•_1ì°¨": ë¬¸ì˜ìœ í˜•_1ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨,
                "ê³ ê°ìœ í˜•_2ì°¨": ê³ ê°ìœ í˜•_2ì°¨,
                "ë¬¸ì˜ìœ í˜•_2ì°¨": ë¬¸ì˜ìœ í˜•_2ì°¨,
                "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨": ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨,
            }
            processed_data.append(processed_item)

        df = pd.DataFrame(processed_data)

        # firstAskedAt ì»¬ëŸ¼ ì „ì²´ íƒ€ì… ê°•ì œë³€í™˜ (ì§„ì§œ ì•ˆì „í•˜ê²Œ!)
        if "firstAskedAt" in df.columns:
            df["firstAskedAt"] = pd.to_datetime(df["firstAskedAt"], errors="coerce")

        # í•„ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ì—†ìœ¼ë©´ ë¹ˆ ê°’ìœ¼ë¡œ ìƒì„±
        required_columns = [
            "ê³ ê°ìœ í˜•", "ë¬¸ì˜ìœ í˜•", "ì„œë¹„ìŠ¤ìœ í˜•",
            "ê³ ê°ìœ í˜•_1ì°¨", "ë¬¸ì˜ìœ í˜•_1ì°¨", "ì„œë¹„ìŠ¤ìœ í˜•_1ì°¨",
            "ê³ ê°ìœ í˜•_2ì°¨", "ë¬¸ì˜ìœ í˜•_2ì°¨", "ì„œë¹„ìŠ¤ìœ í˜•_2ì°¨",
            "firstAskedAt", "operationWaitingTime",
            "operationAvgReplyTime", "operationTotalReplyTime",
            "operationResolutionTime"
        ]

        for col in required_columns:
            if col not in df.columns:
                print(f"[PROCESS] í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½, ë¹ˆ ê°’ìœ¼ë¡œ ìƒì„±: {col}")
                df[col] = None
        
        print(f"[PROCESS] ìµœì¢… ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        print(f"[PROCESS] ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(df)} ê±´")

        return df

channel_api = ChannelTalkAPI()

class ServerCache:
    def __init__(self, cache_dir=None):
        if cache_dir is None:
            self.cache_dir = CACHE_DIR
        else:
            self.cache_dir = cache_dir
        print(f"[CACHE] ServerCache ì´ˆê¸°í™” - ìºì‹œ ë””ë ‰í† ë¦¬: {os.path.abspath(self.cache_dir)}")
        self.ensure_cache_dir()
    
    def ensure_cache_dir(self):
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def get_cache_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def get_metadata_path(self, cache_key: str) -> str:
        return os.path.join(self.cache_dir, f"{cache_key}_metadata.json")
    
    def save_data(self, cache_key: str, data: pd.DataFrame, metadata: Dict):
        print(f"====[save_data ì§„ì…]==== key={cache_key}, ë°ì´í„°ê¸¸ì´={len(data)}")
        print(f"cache_key: {cache_key}")
        print(f"data type: {type(data)}, len: {len(data)}")
        print(f"data columns: {getattr(data, 'columns', 'N/A')}")
        print(f"data.empty: {getattr(data, 'empty', 'N/A')}")
        if hasattr(data, 'head'):
            print(f"data.head():\n{data.head()}")
        
        try:
            self.ensure_cache_dir()
            metadata.update({
                "saved_at": datetime.now().isoformat(),
                "data_count": len(data),
                "cache_version": "1.0"
            })
            
            # ğŸ’¡ firstAskedAt ì»¬ëŸ¼ ì „ì²´ íƒ€ì… ê°•ì œë³€í™˜
            print(f"[DEBUG] 'firstAskedAt' in data.columns: {'firstAskedAt' in data.columns}")
            print(f"[DEBUG] not data.empty: {not data.empty}")
            if "firstAskedAt" in data.columns and not data.empty:
                print(f"[DEBUG] ì›ë³¸ firstAskedAt dtype: {data['firstAskedAt'].dtype}")
                print(f"[DEBUG] ì›ë³¸ firstAskedAt ìƒ˜í”Œ: {data['firstAskedAt'].head(10).tolist()}")
                
                data["firstAskedAt"] = pd.to_datetime(data["firstAskedAt"], errors="coerce")
                print(f"[DEBUG] ë³€í™˜ í›„ firstAskedAt dtype: {data['firstAskedAt'].dtype}")
                print(f"[DEBUG] ë³€í™˜ í›„ firstAskedAt ìƒ˜í”Œ: {data['firstAskedAt'].head(10).tolist()}")
                
                valid_times = data["firstAskedAt"].dropna()
                print(f"[DEBUG] dropna í›„ valid_times ê°œìˆ˜: {len(valid_times)}")
                print(f"[DEBUG] valid_times ìƒ˜í”Œ: {valid_times.head(10).tolist()}")

                def safe_to_iso(val):
                    if isinstance(val, (pd.Timestamp, datetime)):
                        if pd.isna(val):
                            return ""
                        return val.isoformat()
                    if isinstance(val, str):
                        try:
                            dt = pd.to_datetime(val, errors="coerce")
                            if pd.isna(dt):
                                return ""
                            return dt.isoformat()
                        except Exception as e:
                            print(f"[CACHE] safe_to_iso: str ë³€í™˜ ì‹¤íŒ¨ {val}: {e}")
                            return ""
                    return ""
                
                if not valid_times.empty:
                    start_val = valid_times.min()
                    end_val = valid_times.max()
                    print(f"[DEBUG] min: {start_val} (type: {type(start_val)})")
                    print(f"[DEBUG] max: {end_val} (type: {type(end_val)})")
                else:
                    start_val = ""
                    end_val = ""
                    print(f"[DEBUG] valid_timesê°€ ë¹„ì–´ìˆìŒ - ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •")
                
                metadata["first_asked_start"] = safe_to_iso(start_val)
                metadata["first_asked_end"] = safe_to_iso(end_val)
            else:
                print("[DEBUG] firstAskedAt ì¡°ê±´ë¬¸ ì§„ì… ì•ˆë¨ - ì»¬ëŸ¼ ì—†ê±°ë‚˜ ë°ì´í„° ë¹„ì–´ìˆìŒ")

            data_path = self.get_cache_path(cache_key)
            data.to_pickle(data_path)
            metadata_path = self.get_metadata_path(cache_key)
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"[CACHE] ë°ì´í„° ì €ì¥ ì™„ë£Œ: {cache_key}")
            return True
        except Exception as e:
            print(f"[CACHE] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"[CACHE] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    # ì´í•˜ ë™ì¼ (ìƒëµ)
server_cache = ServerCache()
